{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classificação - Manual.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJoSkkYT3nSP",
        "colab_type": "text"
      },
      "source": [
        "###Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL6Ovfo3um1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2EXAePtYrXe",
        "colab_type": "text"
      },
      "source": [
        "#Google Drive e Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njyA7gc-Yu9P",
        "colab_type": "code",
        "outputId": "5ef6e76a-aa11-41a8-a9ab-253aab345dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGLLlJ8aagJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/gdrive/My Drive/Colab Notebooks/DataSets'\n",
        "\n",
        "path_credito = os.path.join(path, 'credit_data.csv')\n",
        "base_credito = pd.read_csv(path_credito)\n",
        "\n",
        "path_census = os.path.join(path, 'census.csv')\n",
        "base_census = pd.read_csv(path_census)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zo6NyLRwGGl",
        "colab_type": "text"
      },
      "source": [
        "#Naive Bayes\n",
        "\n",
        "O algoritmo gera uma tabela de propabilidades e calcula onde o novo registro se encaixa na tabela\n",
        "\n",
        "###Considerações Importantes\n",
        "* Esse modelo não aceita atributos categóricos. Precisamos converte-los em atributos numéricos. Ex: Atributo de classificação de cor (branco, negro, pardo, etc..), iremos converter cada classe em um atributo e atribuir 1, onde o atributo é verdadeiro e 0 para falso.\n",
        "\n",
        "* O modelo não aceita dados faltantes, nulos (NaN)\n",
        "\n",
        "* O escalonamento de dados numéricos não apresentou nenhuma diferença no resultado\n",
        "\n",
        "* Sempre testar as possibilidades:\n",
        "  * LabelEncoder\n",
        "  * LabelEncoder + OneHotEncoder\n",
        "  * LabelEncoder + Escalonamento\n",
        "  * LabelEncoder + OneHotEncoder + Escalonamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNtpQrDa1RDs",
        "colab_type": "text"
      },
      "source": [
        "##1.Base Risco de Créditos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1gyMxx8u4-e",
        "colab_type": "code",
        "outputId": "1f61192c-171e-4e55-e6ed-a966d0d56bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "base_credito.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientid</th>\n",
              "      <th>income</th>\n",
              "      <th>age</th>\n",
              "      <th>loan</th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>66155.925095</td>\n",
              "      <td>59.017015</td>\n",
              "      <td>8106.532131</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>34415.153966</td>\n",
              "      <td>48.117153</td>\n",
              "      <td>6564.745018</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>57317.170063</td>\n",
              "      <td>63.108049</td>\n",
              "      <td>8020.953296</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>42709.534201</td>\n",
              "      <td>45.751972</td>\n",
              "      <td>6103.642260</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>66952.688845</td>\n",
              "      <td>18.584336</td>\n",
              "      <td>8770.099235</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   clientid        income        age         loan  default\n",
              "0         1  66155.925095  59.017015  8106.532131        0\n",
              "1         2  34415.153966  48.117153  6564.745018        0\n",
              "2         3  57317.170063  63.108049  8020.953296        0\n",
              "3         4  42709.534201  45.751972  6103.642260        0\n",
              "4         5  66952.688845  18.584336  8770.099235        1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94KzwPs9Vg-4",
        "colab_type": "text"
      },
      "source": [
        "###Base Line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYoAxECCVkYE",
        "colab_type": "code",
        "outputId": "058a6a30-f3fe-4bf7-b86d-3ccd4560ffa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "collections.Counter(base_credito.iloc[:,4].values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 1717, 1: 283})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgGrPGxPvfFd",
        "colab_type": "text"
      },
      "source": [
        "### Criando Previsores (X)  e classe (y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrovSxI9vAUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = base_credito.iloc[:, 0:4].values\n",
        "classe = base_credito.iloc[:,4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_ZKdk34xaTq",
        "colab_type": "text"
      },
      "source": [
        "### Transformando variáveis categóricas em atributos discretos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uY3YWmHxa4P",
        "colab_type": "code",
        "outputId": "42580839-ad78-48bb-f0bf-448b587974eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "previsores[:, 0] = labelencoder.fit_transform(previsores[:,0])\n",
        "previsores[:, 1] = labelencoder.fit_transform(previsores[:,1])\n",
        "previsores[:, 2] = labelencoder.fit_transform(previsores[:,2])\n",
        "previsores[:, 3] = labelencoder.fit_transform(previsores[:,3])\n",
        "previsores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.000e+00, 1.849e+03, 1.795e+03, 1.730e+03],\n",
              "       [1.000e+00, 5.610e+02, 1.304e+03, 1.530e+03],\n",
              "       [2.000e+00, 1.473e+03, 1.964e+03, 1.718e+03],\n",
              "       ...,\n",
              "       [1.997e+03, 9.440e+02, 4.530e+02, 1.316e+03],\n",
              "       [1.998e+03, 9.210e+02, 1.996e+03, 4.090e+02],\n",
              "       [1.999e+03, 1.983e+03, 1.675e+03, 1.641e+03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtnIlc08vl0F",
        "colab_type": "text"
      },
      "source": [
        "### Importando modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtTD6rntvOlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "classificador = GaussianNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XuUAk4UvuQ2",
        "colab_type": "text"
      },
      "source": [
        "### Gerando a tabela de probabilidade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyUeE9NXvXED",
        "colab_type": "code",
        "outputId": "9aa3741b-295b-43b4-d8cc-5e4609ebfe39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classificador.fit(previsores, classe)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaZAKpCTy48m",
        "colab_type": "text"
      },
      "source": [
        "### Predizendo um resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcxd7QJPxT4B",
        "colab_type": "code",
        "outputId": "6482e0ac-1bc3-4f8d-fd1f-9321273564b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "resultado = classificador.predict([[0,0,1,2]])\n",
        "resultado"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOanOFlLzfSJ",
        "colab_type": "text"
      },
      "source": [
        "### Correção Laplaciana é feita automaticamente - Quando a base de dados é pequena e faltam alguns valores e o calculo da probabilidade de alguma possibilidade é 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOb-dMflzTPA",
        "colab_type": "code",
        "outputId": "ad17cc42-fa61-4624-ba60-8504eb30f16f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "resultado = classificador.predict([[3,0,0,0]])\n",
        "resultado"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osT_r2aoz4e_",
        "colab_type": "code",
        "outputId": "eaed9b19-da02-4875-a523-c61e690463c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classificador.classes_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRu8GMya0DXl",
        "colab_type": "code",
        "outputId": "eb6d758b-05f7-4a40-a9d8-d935256aa587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classificador.class_count_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1717.,  283.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcjyH8ic0FTO",
        "colab_type": "code",
        "outputId": "702d3bd6-0529-4e9e-e2cc-23a6a3446b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classificador.class_prior_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8585, 0.1415])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV7NNA9f1XCF",
        "colab_type": "text"
      },
      "source": [
        "##2.Base Créditos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsTtlyjz0GfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_credito.loc[base_credito.age < 0, 'age'] = 40.92"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiV_pi_B1jZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = base_credito.iloc[:, 1:4].values\n",
        "classe = base_credito.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsNIQ_r0V8Q9",
        "colab_type": "text"
      },
      "source": [
        "###Base Line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTfmUGLdV-EP",
        "colab_type": "code",
        "outputId": "2494e19f-7159-43a4-fb64-81bacc386d24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "classe_count = collections.Counter(classe)\n",
        "\n",
        "base_line = classe_count[0] / len(classe)\n",
        "base_line"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8585"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p07HL1w34ZlJ",
        "colab_type": "text"
      },
      "source": [
        "### Altero valores faltantes em valores reais, utilizando a média dos valores como estratégia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr9bkiia17dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer()\n",
        "imputer = imputer.fit(previsores[:, 1:4])\n",
        "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGeIA05K4Twl",
        "colab_type": "text"
      },
      "source": [
        "### Transforma todos os valores numéricos na mesma escala"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA4GBIkJ180t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk-AFqSB4m_b",
        "colab_type": "text"
      },
      "source": [
        "### Divide a base de dados em treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtpAD--T1_-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDRNdl0U5Jo2",
        "colab_type": "text"
      },
      "source": [
        "### Trinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neg4PeV22A7V",
        "colab_type": "code",
        "outputId": "c188a2ab-7dec-4960-c063-77e075f82fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classificador = GaussianNB()\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egetryns5O93",
        "colab_type": "text"
      },
      "source": [
        "### Predizendo valores de teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF3WBNit5SDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lzjAoAM5TJi",
        "colab_type": "text"
      },
      "source": [
        "### Acurácia e Matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dh3qc7D2CXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO4JYWMp6Ilg",
        "colab_type": "text"
      },
      "source": [
        "### Precisão - Resultado percentual de acertos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROHRYF6p5XnL",
        "colab_type": "code",
        "outputId": "953d0258-1213-4c52-a51d-06a86ab3b8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SBnnL5m6OLN",
        "colab_type": "text"
      },
      "source": [
        "### Matriz de confusão - Comparação de acertos e erros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlHJ-6_35Yhj",
        "colab_type": "code",
        "outputId": "46d1bf36-8755-4596-b123-cfa0ce319dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[428,   8],\n",
              "       [ 23,  41]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5Kx4Dwoj2vt",
        "colab_type": "text"
      },
      "source": [
        "##3.Base Census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIMuGSl7j6el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classe = base_census.iloc[:, 14].values\n",
        "previsores = base_census.iloc[:, 0:14].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK-zhPuej-e2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_previsores = LabelEncoder()\n",
        "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:, 1])\n",
        "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:, 3])\n",
        "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:, 5])\n",
        "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:, 6])\n",
        "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:, 7])\n",
        "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:, 8])\n",
        "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:, 9])\n",
        "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:, 13])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DLD8pRej-cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = pd.get_dummies(pd.DataFrame(previsores), columns=[1,3,5,6,7,8,9,13]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0_cYJFFj-aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_classe = LabelEncoder()\n",
        "classe = labelencoder_classe.fit_transform(classe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee3X7X7Uj-Vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZfh1TlQj-S2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jLTj-ylj-Ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classificador = GaussianNB()\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4hTe-Dzj-LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrryk3uAj9sy",
        "colab_type": "code",
        "outputId": "ae88b1f4-82af-467e-8a8c-b566ac50ef63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz, precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1172, 2521],\n",
              "        [  35, 1157]]), 0.4767656090071648)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpENV2ZILIg7",
        "colab_type": "text"
      },
      "source": [
        "#Árvore de Decisão\n",
        "\n",
        "O algoritmo gera uma árvore com várias condições e percorre essa árvore para classificar o novo registro.\n",
        "\n",
        "##Considerações Importantes\n",
        "\n",
        "* Esse modelo não aceita atributos categóricos. Precisamos converte-los em atributos numéricos. Ex: Atributo de classificação de cor (branco, negro, pardo, etc..), iremos converter cada classe em um atributo e atribuir 1, onde o atributo é verdadeiro e 0 para falso.\n",
        "* Não é necessário fazer o escalonamento dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIkmUETXL-NB",
        "colab_type": "text"
      },
      "source": [
        "##1.Base Créditos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTyNH-C3LK_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_credito.loc[base_credito.age < 0, 'age'] = 40.92"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6alDb55CMQTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = base_credito.iloc[:, 1:4].values\n",
        "classe = base_credito.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNCFCWI1LNKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer()\n",
        "imputer = imputer.fit(previsores[:, 1:4])\n",
        "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P3HXDyjMJf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0-Gz8bCMVb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0AQIHeYMYW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classificador = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ApiyfcWMZyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK8xW_hmNirX",
        "colab_type": "code",
        "outputId": "8dfaed75-3484-4eca-e762-5302a66c9862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtAPcVqWNoAU",
        "colab_type": "code",
        "outputId": "8effe700-237c-421c-9617-ec1153446d8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[430,   6],\n",
              "       [  3,  61]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHVUsDwUNov6",
        "colab_type": "code",
        "outputId": "c955f36d-f026-4ec9-d98d-8c8acb70f326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "import seaborn as sns\n",
        "ax = sns.heatmap(matriz,annot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAFNwMkksM8n",
        "colab_type": "text"
      },
      "source": [
        "##2.Base Census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfEBQq8bsPzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classe = base_census.iloc[:, 14].values\n",
        "previsores = base_census.iloc[:, 0:14].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkHVoycjsV3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_previsores = LabelEncoder()\n",
        "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:, 1])\n",
        "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:, 3])\n",
        "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:, 5])\n",
        "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:, 6])\n",
        "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:, 7])\n",
        "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:, 8])\n",
        "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:, 9])\n",
        "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:, 13])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpvSsrzGse7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = pd.get_dummies(pd.DataFrame(previsores), columns=[1,3,5,6,7,8,9,13]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MgQgukEsfpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_classe = LabelEncoder()\n",
        "classe = labelencoder_classe.fit_transform(classe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwsfcKbYsluw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD7ADzJbsll9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVTYuvBwslY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classificador = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6j8Qns6susU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJcFvZq2s0Nk",
        "colab_type": "code",
        "outputId": "dfe20c28-a88a-4d5d-f1a1-dcd621ee5fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8092118730808597"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIgxfRB8sza9",
        "colab_type": "code",
        "outputId": "784fee5b-c80b-4775-e3e7-5e1617a9ec91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3224,  469],\n",
              "       [ 463,  729]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpi988t1THMu",
        "colab_type": "text"
      },
      "source": [
        "#Random Forest\n",
        "\n",
        "O algoritmo gera várias árvores com várias condições cada, submete o novo registro a cada árvore e classifica com o resultado da maioria das árvores "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0fh3FVGTOUK",
        "colab_type": "text"
      },
      "source": [
        "##1.Base Créditos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOQwYGfbO00T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_credito.loc[base_credito.age < 0, 'age'] = 40.92"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxnm360qTW9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = base_credito.iloc[:, 1:4].values\n",
        "classe = base_credito.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp6M0gjHTXS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer()\n",
        "imputer = imputer.fit(previsores[:, 1:4])\n",
        "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT2f6vdvTZEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDYuMHigTbPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3DKD6P1TeD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classificador = RandomForestClassifier(n_estimators=40, criterion='entropy', random_state=0)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGE3PHbfUGvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcmrulb-UH4I",
        "colab_type": "code",
        "outputId": "2add63ab-7b5f-4fcb-b658-33b09f74fe16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import seaborn as sns\n",
        "ax = sns.heatmap(matriz,annot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45ibCyyIUOxQ",
        "colab_type": "code",
        "outputId": "52d51c1f-2bef-4f4e-a438-558a7ecdcf72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSBZYm8zuo6R",
        "colab_type": "text"
      },
      "source": [
        "##2.Base Census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHQ73012ussU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classe = base_census.iloc[:, 14].values\n",
        "previsores = base_census.iloc[:, 0:14].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9fE-zdmusol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_previsores = LabelEncoder()\n",
        "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:, 1])\n",
        "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:, 3])\n",
        "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:, 5])\n",
        "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:, 6])\n",
        "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:, 7])\n",
        "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:, 8])\n",
        "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:, 9])\n",
        "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:, 13])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29bwAmKsusmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = pd.get_dummies(pd.DataFrame(previsores), columns=[1,3,5,6,7,8,9,13]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIpIbyMFusk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_classe = LabelEncoder()\n",
        "classe = labelencoder_classe.fit_transform(classe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfiWUN_vushp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAVaOwEYuseu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3K4jBXhusb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classificador = RandomForestClassifier(n_estimators=40, criterion='entropy', random_state=0)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COz9kgAZu7iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JNqQGBru7bf",
        "colab_type": "code",
        "outputId": "5a7fbd34-e97e-4da0-c538-3a9f58020bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.849539406345957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmoGeEQbusYw",
        "colab_type": "code",
        "outputId": "e0a8fa3f-47b3-416c-8271-bb77997e5180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3428,  265],\n",
              "       [ 470,  722]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBLIs-V7Hv1X",
        "colab_type": "text"
      },
      "source": [
        "#kNN - k-Nearest Neighbor\n",
        "\n",
        "O algoritmo armazena todos os registros de treinamento e classifica o novo registro calculando a distância dele com os registros armazenados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKqIVtTQmz4h",
        "colab_type": "text"
      },
      "source": [
        "##1.Base Créditos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V0tnFjWkVnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_credito.loc[base_credito.age < 0, 'age'] = 40.92\n",
        "previsores = base_credito.iloc[:, 1:4].values\n",
        "classe = base_credito.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JYutxe_-Ub3",
        "colab_type": "code",
        "outputId": "d0680bcd-3678-4f0e-a3dc-69cb66c84d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "base_credito.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientid</th>\n",
              "      <th>income</th>\n",
              "      <th>age</th>\n",
              "      <th>loan</th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>66155.925095</td>\n",
              "      <td>59.017015</td>\n",
              "      <td>8106.532131</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>34415.153966</td>\n",
              "      <td>48.117153</td>\n",
              "      <td>6564.745018</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>57317.170063</td>\n",
              "      <td>63.108049</td>\n",
              "      <td>8020.953296</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>42709.534201</td>\n",
              "      <td>45.751972</td>\n",
              "      <td>6103.642260</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>66952.688845</td>\n",
              "      <td>18.584336</td>\n",
              "      <td>8770.099235</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   clientid        income        age         loan  default\n",
              "0         1  66155.925095  59.017015  8106.532131        0\n",
              "1         2  34415.153966  48.117153  6564.745018        0\n",
              "2         3  57317.170063  63.108049  8020.953296        0\n",
              "3         4  42709.534201  45.751972  6103.642260        0\n",
              "4         5  66952.688845  18.584336  8770.099235        1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WF3EloxkX_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer()\n",
        "imputer = imputer.fit(previsores[:, 1:4])\n",
        "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAA54-h5kZw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSG-6c1rkc8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJvLlZSake7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "classificador = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsot4hcCkluC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CanZkkYPkpEG",
        "colab_type": "code",
        "outputId": "d8917106-cc9d-4052-e3bc-f509794fb12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import seaborn as sns\n",
        "ax = sns.heatmap(matriz,annot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZeCbzTskptc",
        "colab_type": "code",
        "outputId": "b2d6002a-2ff6-4d8c-c641-5cd97b5251d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaENvvhAm2cJ",
        "colab_type": "text"
      },
      "source": [
        "##2.Base Census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVbstvFYlheS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classe = base_census.iloc[:, 14].values\n",
        "previsores = base_census.iloc[:, 0:14].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zflXFVDsoqew",
        "colab_type": "text"
      },
      "source": [
        "###Transforma as variáveis categóricas em variáveis numéricas. A função LabelEncoder enumera cada categoria.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR9F9Mn-m4Ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_previsores = LabelEncoder()\n",
        "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:, 1])\n",
        "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:, 3])\n",
        "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:, 5])\n",
        "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:, 6])\n",
        "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:, 7])\n",
        "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:, 8])\n",
        "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:, 9])\n",
        "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:, 13])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzhzNI8toV4a",
        "colab_type": "code",
        "outputId": "3550174c-6dd2-41d7-82d2-9e977e846e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "previsores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[39, 7, 77516, ..., 0, 40, 39],\n",
              "       [50, 6, 83311, ..., 0, 13, 39],\n",
              "       [38, 4, 215646, ..., 0, 40, 39],\n",
              "       ...,\n",
              "       [58, 4, 151910, ..., 0, 40, 39],\n",
              "       [22, 4, 201490, ..., 0, 20, 39],\n",
              "       [52, 5, 287927, ..., 0, 40, 39]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qys82d_f_kG8",
        "colab_type": "text"
      },
      "source": [
        "Para não considerarmos pesos para as categorias enumeradas, transformamos ela em um array binário, onde todas as colunas serão preenchidos por 0, menos a coluna correspondente a classe atribuída a variável."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjg3aK3row6h",
        "colab_type": "text"
      },
      "source": [
        "###Deixa todas as variáveis numéricas, que eram categóricas, com o mesmo peso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pf3uaqB5nve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfora a coluna em varias colunas \n",
        "previsores = pd.get_dummies(pd.DataFrame(previsores), columns=[1,3,5,6,7,8,9,13]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4kXe5vfcwG8",
        "colab_type": "code",
        "outputId": "da040be0-11dc-44ed-f732-5639069856e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "previsores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[39, 77516, 13, ..., 1, 0, 0],\n",
              "       [50, 83311, 13, ..., 1, 0, 0],\n",
              "       [38, 215646, 9, ..., 1, 0, 0],\n",
              "       ...,\n",
              "       [58, 151910, 9, ..., 1, 0, 0],\n",
              "       [22, 201490, 9, ..., 1, 0, 0],\n",
              "       [52, 287927, 9, ..., 1, 0, 0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nok32xpNdnek",
        "colab_type": "code",
        "outputId": "5b3a6c62-553f-43dd-bc18-ed311bb563da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classe.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32561,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQB4FOWlduvl",
        "colab_type": "code",
        "outputId": "7cc83441-84e4-41cb-d546-be299cdea138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "previsores.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32561, 108)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtSOkgqqn1Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_classe = LabelEncoder()\n",
        "classe = labelencoder_classe.fit_transform(classe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrjiky3vn9Nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb0O9rffn_tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h5YBsdaoBD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classificador = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MEO0uhsoCYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34_FeutNoDtT",
        "colab_type": "code",
        "outputId": "71e05630-9bd1-4020-90d3-70a994c2fad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3336,  357],\n",
              "       [ 511,  681]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq8X9y8He0eO",
        "colab_type": "code",
        "outputId": "ebbe6606-ae17-46d3-beb4-6fc269102823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8223132036847492"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaRtkD49YHxR",
        "colab_type": "text"
      },
      "source": [
        "#Regressão Logística\n",
        "\n",
        "Algoritmo baseado na regressão linear. Ele calcula o sigmóide e traça uma linha para dividir e classificar o novo registro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0eTZ0fBYMG0",
        "colab_type": "text"
      },
      "source": [
        "##1.Base Créditos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H-vbeY7ciM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_credito.loc[base_credito.age < 0, 'age'] = 40.92\n",
        "previsores = base_credito.iloc[:, 1:4].values\n",
        "classe = base_credito.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "486G-VumcjPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer()\n",
        "imputer = imputer.fit(previsores[:, 1:4])\n",
        "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7zYYMKTcjNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMruVsgNcjK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLKoQbFRcjIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classificador = LogisticRegression(random_state=1)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O02xP3ocjFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86cdzYS0cw-d",
        "colab_type": "code",
        "outputId": "d22aa9a2-8c28-4d03-aef7-6b237546ddba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import seaborn as sns\n",
        "ax = sns.heatmap(matriz,annot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpoLQar6cy_-",
        "colab_type": "code",
        "outputId": "43abea7a-c7bc-4bd2-eaa9-d0845386e83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwdo_yu7YP16",
        "colab_type": "text"
      },
      "source": [
        "##2.Base Census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tHLFnBZe8Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classe = base_census.iloc[:, 14].values\n",
        "previsores = base_census.iloc[:, 0:14].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knO-lL-ZfCp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_previsores = LabelEncoder()\n",
        "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:, 1])\n",
        "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:, 3])\n",
        "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:, 5])\n",
        "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:, 6])\n",
        "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:, 7])\n",
        "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:, 8])\n",
        "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:, 9])\n",
        "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:, 13])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1nSdJNPfF37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = pd.get_dummies(pd.DataFrame(previsores), columns=[1,3,5,6,7,8,9,13]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9FSxenLfJb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_classe = LabelEncoder()\n",
        "classe = labelencoder_classe.fit_transform(classe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASvGVBjfLhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou0n8NY5fNiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94lTV_CwfPhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classificador = LogisticRegression()\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D75qmYMCfZSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQhdpP7rffUK",
        "colab_type": "code",
        "outputId": "25258e08-17d6-4a65-a5f8-c612e8c71a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3423,  270],\n",
              "       [ 465,  727]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KISdHfELff-5",
        "colab_type": "code",
        "outputId": "045a6e06-5620-4de3-b53c-602e96c8e964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.849539406345957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKyl1JyolSCl",
        "colab_type": "text"
      },
      "source": [
        "#SVM - Vetores de Suporte de Maquina\n",
        "\n",
        "O algoritmo traça várias retas e encontra a melhor reta baseado na máxima distância da reta para os elementos mais próximos a reta.\n",
        "Para problemas não lineares, o algoritmo utiliza um método chamado Kernel Trick para separar os dados de acordo que consigamos traçar uma reta\n",
        ".\n",
        "\n",
        "* Sempre utilizar o hot encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm0ymMoktQxM",
        "colab_type": "text"
      },
      "source": [
        "##1.Base Créditos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR0PRv4BlT8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_credito.loc[base_credito.age < 0, 'age'] = 40.92\n",
        "previsores = base_credito.iloc[:, 1:4].values\n",
        "classe = base_credito.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3SmoSQKtcmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer()\n",
        "imputer = imputer.fit(previsores[:, 1:4])\n",
        "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzLwPrfetcjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-MpOx9utcfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk-ZkavEuGR_",
        "colab_type": "text"
      },
      "source": [
        "* Parâmetro 'kernel' é onde ele transforma os dados lineares em não lineares\n",
        "* Parâmetro 'c' é o custo. A tendência é termos resultados melhores com custo mais alto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK4_pxRktccX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "classificador = SVC(kernel='rbf', C=2)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqjkuSFOtcZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgTJw96ytly8",
        "colab_type": "code",
        "outputId": "b0ba195a-47bf-43f9-d77a-2b7a8a0348a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import seaborn as sns\n",
        "ax = sns.heatmap(matriz,annot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJjIjBqEtnkl",
        "colab_type": "code",
        "outputId": "1dbf2e4b-2d79-464c-eb0e-86e363edf7f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.988"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y2v5VwGtTv1",
        "colab_type": "text"
      },
      "source": [
        "##2.Base Census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrCp8FSctU_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classe = base_census.iloc[:, 14].values\n",
        "previsores = base_census.iloc[:, 0:14].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocWsQ0NNvzF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_previsores = LabelEncoder()\n",
        "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:, 1])\n",
        "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:, 3])\n",
        "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:, 5])\n",
        "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:, 6])\n",
        "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:, 7])\n",
        "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:, 8])\n",
        "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:, 9])\n",
        "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:, 13])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LBMKKwUv1ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = pd.get_dummies(pd.DataFrame(previsores), columns=[1,3,5,6,7,8,9,13]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcs4Xlarv2-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_classe = LabelEncoder()\n",
        "classe = labelencoder_classe.fit_transform(classe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1HK8RbKv5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7j6RXZlv7kO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVVoufAtw79C",
        "colab_type": "text"
      },
      "source": [
        "* Parâmetro 'kernel' é onde ele transforma os dados lineares em não lineares\n",
        "* Parâmetro 'c' é o custo. A tendência é termos resultados melhores com custo mais alto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYnsv8idv9PX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "classificador = SVC(kernel='linear')\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9swH1g2v_FT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln3NIH02wBh1",
        "colab_type": "code",
        "outputId": "c9a6c07b-893c-49d5-a589-5bbbeedde930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3459,  234],\n",
              "       [ 495,  697]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mElWkbRHwCQy",
        "colab_type": "code",
        "outputId": "8519e077-efce-4c17-f977-eac80dc727b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8507676560900717"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJeYp1-vyyTs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KKAcnMLlGZH",
        "colab_type": "text"
      },
      "source": [
        "#Redes Neurais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA2DDEV2lKmx",
        "colab_type": "text"
      },
      "source": [
        "As redes neurais são treinadas para que se ajustem os pesos (w) de cada atributo (x). No final da rede existe uma função soma, que gera o resultado para a função de ativação, baseado nas entradas e pesos. A função de ativação calcula o resultado final da rede neural. Existema várias funções de ativação e a mais famosa é a Sigmoide.\n",
        "\n",
        "Após isso, precisamos calculal a função do erro, para verificar se os pesos estão ajustados ou não:\n",
        "* Erro Simples\n",
        "> $erro = 1 - FunçãoAtivação$\n",
        "* Mean Square Error\n",
        "> $MSE=\\frac{1}{N}\\sum_{i=1}^N(f_i - y_i)^2$\n",
        "* Root Mean Square Error\n",
        "> $RMSE=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(f_i - y_i)^2}$\n",
        "\n",
        "Para isso, calculamos o deltada função de saída:\n",
        "\n",
        "> $DeltaSaída = Erro * DerivadaSigmoide$\n",
        "\n",
        "E a o delta para a função da camada oculta:\n",
        "\n",
        "> DeltaCamadaOculta = DerivadaSigmoide * peso * DeltaSaida\n",
        "\n",
        ">$peso_{n+1}=(peso_n * momento) + (entrada * delta * TaxaApendizagem)$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4sswiaQsdBN",
        "colab_type": "text"
      },
      "source": [
        "##1.Base Créditos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTdKRG4wwDAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_credito.loc[base_credito.age < 0, 'age'] = 40.92\n",
        "previsores = base_credito.iloc[:, 1:4].values\n",
        "classe = base_credito.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQUpXzFWs6-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer()\n",
        "imputer = imputer.fit(previsores[:, 1:4])\n",
        "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZj01ALFs9kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0AD25mWtA_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiSjObTlvHdF",
        "colab_type": "text"
      },
      "source": [
        "####Parâmetros Importantes:\n",
        "* verbose: Mostra o erro a cada iteração.\n",
        "* max_iter: Máximo de iterações no ajuste dos pesos.\n",
        "* tol: Valor de tolerância do ajuste do erro. Caso o erro não melhore mais que o valor da tolerância, ele interrompe as iterações. \n",
        "* solver: Algoritmo usado para o ajuste dos pesos.\n",
        "* hidden_layers_size: Número de camadas ocultas.\n",
        "* activation: Função de aticação.\n",
        "* batch_size: Tamanho do conjunto de registros utilizados por vez para fazer a atualização dos pesos.\n",
        "* learning_rate: Taxa de aprendizado.\n",
        "* momentum: Utilizado para a decida do gradiente descendente estocástico, quando o parâmentro solver estiver setado como sgd.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls3EhiTqtJJr",
        "colab_type": "code",
        "outputId": "b509bb8e-226d-465f-9ed8-e935b3dfcb89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "classificador = MLPClassifier(verbose=True, max_iter=1000, tol=0.0000010, solver='adam', hidden_layer_sizes=(80), activation='relu')\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.77418116\n",
            "Iteration 2, loss = 0.69675953\n",
            "Iteration 3, loss = 0.62559032\n",
            "Iteration 4, loss = 0.56693583\n",
            "Iteration 5, loss = 0.51433311\n",
            "Iteration 6, loss = 0.46905519\n",
            "Iteration 7, loss = 0.42978637\n",
            "Iteration 8, loss = 0.39630029\n",
            "Iteration 9, loss = 0.36630207\n",
            "Iteration 10, loss = 0.34045477\n",
            "Iteration 11, loss = 0.31777405\n",
            "Iteration 12, loss = 0.29759674\n",
            "Iteration 13, loss = 0.28010681\n",
            "Iteration 14, loss = 0.26431858\n",
            "Iteration 15, loss = 0.25040483\n",
            "Iteration 16, loss = 0.23780282\n",
            "Iteration 17, loss = 0.22653367\n",
            "Iteration 18, loss = 0.21622429\n",
            "Iteration 19, loss = 0.20704980\n",
            "Iteration 20, loss = 0.19861199\n",
            "Iteration 21, loss = 0.19096093\n",
            "Iteration 22, loss = 0.18394202\n",
            "Iteration 23, loss = 0.17750632\n",
            "Iteration 24, loss = 0.17153850\n",
            "Iteration 25, loss = 0.16609065\n",
            "Iteration 26, loss = 0.16106110\n",
            "Iteration 27, loss = 0.15627230\n",
            "Iteration 28, loss = 0.15189087\n",
            "Iteration 29, loss = 0.14778258\n",
            "Iteration 30, loss = 0.14391528\n",
            "Iteration 31, loss = 0.14031620\n",
            "Iteration 32, loss = 0.13692170\n",
            "Iteration 33, loss = 0.13381682\n",
            "Iteration 34, loss = 0.13085214\n",
            "Iteration 35, loss = 0.12803899\n",
            "Iteration 36, loss = 0.12535384\n",
            "Iteration 37, loss = 0.12293889\n",
            "Iteration 38, loss = 0.12055616\n",
            "Iteration 39, loss = 0.11827234\n",
            "Iteration 40, loss = 0.11611793\n",
            "Iteration 41, loss = 0.11400192\n",
            "Iteration 42, loss = 0.11209420\n",
            "Iteration 43, loss = 0.11013036\n",
            "Iteration 44, loss = 0.10823740\n",
            "Iteration 45, loss = 0.10661168\n",
            "Iteration 46, loss = 0.10493795\n",
            "Iteration 47, loss = 0.10325516\n",
            "Iteration 48, loss = 0.10170907\n",
            "Iteration 49, loss = 0.10014029\n",
            "Iteration 50, loss = 0.09862081\n",
            "Iteration 51, loss = 0.09718643\n",
            "Iteration 52, loss = 0.09580297\n",
            "Iteration 53, loss = 0.09451965\n",
            "Iteration 54, loss = 0.09324656\n",
            "Iteration 55, loss = 0.09197927\n",
            "Iteration 56, loss = 0.09066988\n",
            "Iteration 57, loss = 0.08947299\n",
            "Iteration 58, loss = 0.08827196\n",
            "Iteration 59, loss = 0.08725065\n",
            "Iteration 60, loss = 0.08601203\n",
            "Iteration 61, loss = 0.08488223\n",
            "Iteration 62, loss = 0.08382676\n",
            "Iteration 63, loss = 0.08274810\n",
            "Iteration 64, loss = 0.08173525\n",
            "Iteration 65, loss = 0.08077428\n",
            "Iteration 66, loss = 0.07981498\n",
            "Iteration 67, loss = 0.07894924\n",
            "Iteration 68, loss = 0.07803256\n",
            "Iteration 69, loss = 0.07714172\n",
            "Iteration 70, loss = 0.07627129\n",
            "Iteration 71, loss = 0.07549000\n",
            "Iteration 72, loss = 0.07462099\n",
            "Iteration 73, loss = 0.07382640\n",
            "Iteration 74, loss = 0.07297690\n",
            "Iteration 75, loss = 0.07215125\n",
            "Iteration 76, loss = 0.07137768\n",
            "Iteration 77, loss = 0.07068874\n",
            "Iteration 78, loss = 0.06987917\n",
            "Iteration 79, loss = 0.06911285\n",
            "Iteration 80, loss = 0.06853533\n",
            "Iteration 81, loss = 0.06771320\n",
            "Iteration 82, loss = 0.06706354\n",
            "Iteration 83, loss = 0.06636400\n",
            "Iteration 84, loss = 0.06574365\n",
            "Iteration 85, loss = 0.06510481\n",
            "Iteration 86, loss = 0.06448008\n",
            "Iteration 87, loss = 0.06397708\n",
            "Iteration 88, loss = 0.06332134\n",
            "Iteration 89, loss = 0.06273942\n",
            "Iteration 90, loss = 0.06219605\n",
            "Iteration 91, loss = 0.06168074\n",
            "Iteration 92, loss = 0.06116318\n",
            "Iteration 93, loss = 0.06053300\n",
            "Iteration 94, loss = 0.06005046\n",
            "Iteration 95, loss = 0.05954128\n",
            "Iteration 96, loss = 0.05906125\n",
            "Iteration 97, loss = 0.05862148\n",
            "Iteration 98, loss = 0.05807543\n",
            "Iteration 99, loss = 0.05760648\n",
            "Iteration 100, loss = 0.05718060\n",
            "Iteration 101, loss = 0.05667816\n",
            "Iteration 102, loss = 0.05619822\n",
            "Iteration 103, loss = 0.05578013\n",
            "Iteration 104, loss = 0.05529772\n",
            "Iteration 105, loss = 0.05488347\n",
            "Iteration 106, loss = 0.05460290\n",
            "Iteration 107, loss = 0.05409651\n",
            "Iteration 108, loss = 0.05373867\n",
            "Iteration 109, loss = 0.05328519\n",
            "Iteration 110, loss = 0.05286997\n",
            "Iteration 111, loss = 0.05244767\n",
            "Iteration 112, loss = 0.05208685\n",
            "Iteration 113, loss = 0.05170095\n",
            "Iteration 114, loss = 0.05140673\n",
            "Iteration 115, loss = 0.05090530\n",
            "Iteration 116, loss = 0.05054751\n",
            "Iteration 117, loss = 0.05026740\n",
            "Iteration 118, loss = 0.04986161\n",
            "Iteration 119, loss = 0.04954462\n",
            "Iteration 120, loss = 0.04919616\n",
            "Iteration 121, loss = 0.04890298\n",
            "Iteration 122, loss = 0.04840902\n",
            "Iteration 123, loss = 0.04806900\n",
            "Iteration 124, loss = 0.04778219\n",
            "Iteration 125, loss = 0.04741447\n",
            "Iteration 126, loss = 0.04711119\n",
            "Iteration 127, loss = 0.04678571\n",
            "Iteration 128, loss = 0.04650167\n",
            "Iteration 129, loss = 0.04616492\n",
            "Iteration 130, loss = 0.04587992\n",
            "Iteration 131, loss = 0.04557659\n",
            "Iteration 132, loss = 0.04523233\n",
            "Iteration 133, loss = 0.04491140\n",
            "Iteration 134, loss = 0.04463813\n",
            "Iteration 135, loss = 0.04436357\n",
            "Iteration 136, loss = 0.04404584\n",
            "Iteration 137, loss = 0.04383173\n",
            "Iteration 138, loss = 0.04346002\n",
            "Iteration 139, loss = 0.04316578\n",
            "Iteration 140, loss = 0.04292468\n",
            "Iteration 141, loss = 0.04264946\n",
            "Iteration 142, loss = 0.04235958\n",
            "Iteration 143, loss = 0.04215337\n",
            "Iteration 144, loss = 0.04185751\n",
            "Iteration 145, loss = 0.04161454\n",
            "Iteration 146, loss = 0.04137041\n",
            "Iteration 147, loss = 0.04109684\n",
            "Iteration 148, loss = 0.04078533\n",
            "Iteration 149, loss = 0.04060973\n",
            "Iteration 150, loss = 0.04037872\n",
            "Iteration 151, loss = 0.04014992\n",
            "Iteration 152, loss = 0.03976800\n",
            "Iteration 153, loss = 0.03956415\n",
            "Iteration 154, loss = 0.03948088\n",
            "Iteration 155, loss = 0.03922826\n",
            "Iteration 156, loss = 0.03888638\n",
            "Iteration 157, loss = 0.03871995\n",
            "Iteration 158, loss = 0.03847907\n",
            "Iteration 159, loss = 0.03827520\n",
            "Iteration 160, loss = 0.03802110\n",
            "Iteration 161, loss = 0.03777216\n",
            "Iteration 162, loss = 0.03765429\n",
            "Iteration 163, loss = 0.03760406\n",
            "Iteration 164, loss = 0.03716768\n",
            "Iteration 165, loss = 0.03706882\n",
            "Iteration 166, loss = 0.03675627\n",
            "Iteration 167, loss = 0.03660408\n",
            "Iteration 168, loss = 0.03636639\n",
            "Iteration 169, loss = 0.03616619\n",
            "Iteration 170, loss = 0.03589364\n",
            "Iteration 171, loss = 0.03576604\n",
            "Iteration 172, loss = 0.03561317\n",
            "Iteration 173, loss = 0.03534173\n",
            "Iteration 174, loss = 0.03521124\n",
            "Iteration 175, loss = 0.03496309\n",
            "Iteration 176, loss = 0.03477085\n",
            "Iteration 177, loss = 0.03464911\n",
            "Iteration 178, loss = 0.03461846\n",
            "Iteration 179, loss = 0.03424381\n",
            "Iteration 180, loss = 0.03410487\n",
            "Iteration 181, loss = 0.03390548\n",
            "Iteration 182, loss = 0.03376813\n",
            "Iteration 183, loss = 0.03354060\n",
            "Iteration 184, loss = 0.03335764\n",
            "Iteration 185, loss = 0.03322638\n",
            "Iteration 186, loss = 0.03315111\n",
            "Iteration 187, loss = 0.03291185\n",
            "Iteration 188, loss = 0.03267311\n",
            "Iteration 189, loss = 0.03259320\n",
            "Iteration 190, loss = 0.03247969\n",
            "Iteration 191, loss = 0.03236459\n",
            "Iteration 192, loss = 0.03204827\n",
            "Iteration 193, loss = 0.03190314\n",
            "Iteration 194, loss = 0.03182247\n",
            "Iteration 195, loss = 0.03159605\n",
            "Iteration 196, loss = 0.03153788\n",
            "Iteration 197, loss = 0.03133260\n",
            "Iteration 198, loss = 0.03113352\n",
            "Iteration 199, loss = 0.03101917\n",
            "Iteration 200, loss = 0.03086176\n",
            "Iteration 201, loss = 0.03069362\n",
            "Iteration 202, loss = 0.03055308\n",
            "Iteration 203, loss = 0.03044303\n",
            "Iteration 204, loss = 0.03032379\n",
            "Iteration 205, loss = 0.03009340\n",
            "Iteration 206, loss = 0.03005291\n",
            "Iteration 207, loss = 0.02985338\n",
            "Iteration 208, loss = 0.02969081\n",
            "Iteration 209, loss = 0.02960875\n",
            "Iteration 210, loss = 0.02947043\n",
            "Iteration 211, loss = 0.02932177\n",
            "Iteration 212, loss = 0.02918394\n",
            "Iteration 213, loss = 0.02902578\n",
            "Iteration 214, loss = 0.02889562\n",
            "Iteration 215, loss = 0.02879631\n",
            "Iteration 216, loss = 0.02873351\n",
            "Iteration 217, loss = 0.02854647\n",
            "Iteration 218, loss = 0.02840438\n",
            "Iteration 219, loss = 0.02833006\n",
            "Iteration 220, loss = 0.02818861\n",
            "Iteration 221, loss = 0.02803759\n",
            "Iteration 222, loss = 0.02793419\n",
            "Iteration 223, loss = 0.02787442\n",
            "Iteration 224, loss = 0.02770062\n",
            "Iteration 225, loss = 0.02758892\n",
            "Iteration 226, loss = 0.02746317\n",
            "Iteration 227, loss = 0.02742388\n",
            "Iteration 228, loss = 0.02725463\n",
            "Iteration 229, loss = 0.02710959\n",
            "Iteration 230, loss = 0.02714149\n",
            "Iteration 231, loss = 0.02690994\n",
            "Iteration 232, loss = 0.02679203\n",
            "Iteration 233, loss = 0.02666492\n",
            "Iteration 234, loss = 0.02657681\n",
            "Iteration 235, loss = 0.02649161\n",
            "Iteration 236, loss = 0.02637468\n",
            "Iteration 237, loss = 0.02625041\n",
            "Iteration 238, loss = 0.02617960\n",
            "Iteration 239, loss = 0.02602138\n",
            "Iteration 240, loss = 0.02597403\n",
            "Iteration 241, loss = 0.02583397\n",
            "Iteration 242, loss = 0.02574933\n",
            "Iteration 243, loss = 0.02560814\n",
            "Iteration 244, loss = 0.02553054\n",
            "Iteration 245, loss = 0.02545463\n",
            "Iteration 246, loss = 0.02536172\n",
            "Iteration 247, loss = 0.02525661\n",
            "Iteration 248, loss = 0.02515868\n",
            "Iteration 249, loss = 0.02502232\n",
            "Iteration 250, loss = 0.02497318\n",
            "Iteration 251, loss = 0.02489528\n",
            "Iteration 252, loss = 0.02474265\n",
            "Iteration 253, loss = 0.02466482\n",
            "Iteration 254, loss = 0.02476013\n",
            "Iteration 255, loss = 0.02455847\n",
            "Iteration 256, loss = 0.02436356\n",
            "Iteration 257, loss = 0.02422967\n",
            "Iteration 258, loss = 0.02423375\n",
            "Iteration 259, loss = 0.02409716\n",
            "Iteration 260, loss = 0.02405774\n",
            "Iteration 261, loss = 0.02406239\n",
            "Iteration 262, loss = 0.02384772\n",
            "Iteration 263, loss = 0.02388923\n",
            "Iteration 264, loss = 0.02359790\n",
            "Iteration 265, loss = 0.02354343\n",
            "Iteration 266, loss = 0.02347485\n",
            "Iteration 267, loss = 0.02343102\n",
            "Iteration 268, loss = 0.02332760\n",
            "Iteration 269, loss = 0.02320029\n",
            "Iteration 270, loss = 0.02317236\n",
            "Iteration 271, loss = 0.02308813\n",
            "Iteration 272, loss = 0.02294797\n",
            "Iteration 273, loss = 0.02286757\n",
            "Iteration 274, loss = 0.02284173\n",
            "Iteration 275, loss = 0.02272524\n",
            "Iteration 276, loss = 0.02262166\n",
            "Iteration 277, loss = 0.02255100\n",
            "Iteration 278, loss = 0.02250137\n",
            "Iteration 279, loss = 0.02240601\n",
            "Iteration 280, loss = 0.02236991\n",
            "Iteration 281, loss = 0.02233237\n",
            "Iteration 282, loss = 0.02216301\n",
            "Iteration 283, loss = 0.02211746\n",
            "Iteration 284, loss = 0.02200815\n",
            "Iteration 285, loss = 0.02193317\n",
            "Iteration 286, loss = 0.02190303\n",
            "Iteration 287, loss = 0.02181232\n",
            "Iteration 288, loss = 0.02174452\n",
            "Iteration 289, loss = 0.02171582\n",
            "Iteration 290, loss = 0.02153119\n",
            "Iteration 291, loss = 0.02147590\n",
            "Iteration 292, loss = 0.02150859\n",
            "Iteration 293, loss = 0.02139245\n",
            "Iteration 294, loss = 0.02127756\n",
            "Iteration 295, loss = 0.02122159\n",
            "Iteration 296, loss = 0.02112465\n",
            "Iteration 297, loss = 0.02109087\n",
            "Iteration 298, loss = 0.02096438\n",
            "Iteration 299, loss = 0.02094708\n",
            "Iteration 300, loss = 0.02090751\n",
            "Iteration 301, loss = 0.02082252\n",
            "Iteration 302, loss = 0.02069081\n",
            "Iteration 303, loss = 0.02062141\n",
            "Iteration 304, loss = 0.02057345\n",
            "Iteration 305, loss = 0.02056247\n",
            "Iteration 306, loss = 0.02043115\n",
            "Iteration 307, loss = 0.02048587\n",
            "Iteration 308, loss = 0.02034857\n",
            "Iteration 309, loss = 0.02027316\n",
            "Iteration 310, loss = 0.02014768\n",
            "Iteration 311, loss = 0.02014132\n",
            "Iteration 312, loss = 0.02006275\n",
            "Iteration 313, loss = 0.02005660\n",
            "Iteration 314, loss = 0.01992409\n",
            "Iteration 315, loss = 0.01988063\n",
            "Iteration 316, loss = 0.01979229\n",
            "Iteration 317, loss = 0.01977121\n",
            "Iteration 318, loss = 0.01965573\n",
            "Iteration 319, loss = 0.01963093\n",
            "Iteration 320, loss = 0.01960302\n",
            "Iteration 321, loss = 0.01954407\n",
            "Iteration 322, loss = 0.01940533\n",
            "Iteration 323, loss = 0.01935809\n",
            "Iteration 324, loss = 0.01931891\n",
            "Iteration 325, loss = 0.01922936\n",
            "Iteration 326, loss = 0.01919937\n",
            "Iteration 327, loss = 0.01912164\n",
            "Iteration 328, loss = 0.01907274\n",
            "Iteration 329, loss = 0.01910445\n",
            "Iteration 330, loss = 0.01900398\n",
            "Iteration 331, loss = 0.01892324\n",
            "Iteration 332, loss = 0.01888316\n",
            "Iteration 333, loss = 0.01881049\n",
            "Iteration 334, loss = 0.01873255\n",
            "Iteration 335, loss = 0.01866653\n",
            "Iteration 336, loss = 0.01861614\n",
            "Iteration 337, loss = 0.01853573\n",
            "Iteration 338, loss = 0.01852813\n",
            "Iteration 339, loss = 0.01847772\n",
            "Iteration 340, loss = 0.01840409\n",
            "Iteration 341, loss = 0.01835084\n",
            "Iteration 342, loss = 0.01831519\n",
            "Iteration 343, loss = 0.01825803\n",
            "Iteration 344, loss = 0.01823785\n",
            "Iteration 345, loss = 0.01815488\n",
            "Iteration 346, loss = 0.01805638\n",
            "Iteration 347, loss = 0.01799962\n",
            "Iteration 348, loss = 0.01797974\n",
            "Iteration 349, loss = 0.01788692\n",
            "Iteration 350, loss = 0.01787114\n",
            "Iteration 351, loss = 0.01785182\n",
            "Iteration 352, loss = 0.01792680\n",
            "Iteration 353, loss = 0.01779372\n",
            "Iteration 354, loss = 0.01765227\n",
            "Iteration 355, loss = 0.01759677\n",
            "Iteration 356, loss = 0.01758918\n",
            "Iteration 357, loss = 0.01752186\n",
            "Iteration 358, loss = 0.01744687\n",
            "Iteration 359, loss = 0.01741754\n",
            "Iteration 360, loss = 0.01736992\n",
            "Iteration 361, loss = 0.01728369\n",
            "Iteration 362, loss = 0.01723715\n",
            "Iteration 363, loss = 0.01722510\n",
            "Iteration 364, loss = 0.01719040\n",
            "Iteration 365, loss = 0.01712200\n",
            "Iteration 366, loss = 0.01703968\n",
            "Iteration 367, loss = 0.01697194\n",
            "Iteration 368, loss = 0.01714068\n",
            "Iteration 369, loss = 0.01693912\n",
            "Iteration 370, loss = 0.01699539\n",
            "Iteration 371, loss = 0.01685440\n",
            "Iteration 372, loss = 0.01681240\n",
            "Iteration 373, loss = 0.01674238\n",
            "Iteration 374, loss = 0.01669016\n",
            "Iteration 375, loss = 0.01663484\n",
            "Iteration 376, loss = 0.01658103\n",
            "Iteration 377, loss = 0.01656704\n",
            "Iteration 378, loss = 0.01657945\n",
            "Iteration 379, loss = 0.01642113\n",
            "Iteration 380, loss = 0.01648379\n",
            "Iteration 381, loss = 0.01635102\n",
            "Iteration 382, loss = 0.01632447\n",
            "Iteration 383, loss = 0.01630415\n",
            "Iteration 384, loss = 0.01631393\n",
            "Iteration 385, loss = 0.01621519\n",
            "Iteration 386, loss = 0.01619667\n",
            "Iteration 387, loss = 0.01623313\n",
            "Iteration 388, loss = 0.01605295\n",
            "Iteration 389, loss = 0.01604511\n",
            "Iteration 390, loss = 0.01602874\n",
            "Iteration 391, loss = 0.01594812\n",
            "Iteration 392, loss = 0.01595924\n",
            "Iteration 393, loss = 0.01583515\n",
            "Iteration 394, loss = 0.01578489\n",
            "Iteration 395, loss = 0.01578339\n",
            "Iteration 396, loss = 0.01568642\n",
            "Iteration 397, loss = 0.01571331\n",
            "Iteration 398, loss = 0.01557341\n",
            "Iteration 399, loss = 0.01571265\n",
            "Iteration 400, loss = 0.01555502\n",
            "Iteration 401, loss = 0.01564970\n",
            "Iteration 402, loss = 0.01547253\n",
            "Iteration 403, loss = 0.01544027\n",
            "Iteration 404, loss = 0.01544125\n",
            "Iteration 405, loss = 0.01534040\n",
            "Iteration 406, loss = 0.01527398\n",
            "Iteration 407, loss = 0.01524713\n",
            "Iteration 408, loss = 0.01529324\n",
            "Iteration 409, loss = 0.01517499\n",
            "Iteration 410, loss = 0.01515483\n",
            "Iteration 411, loss = 0.01514613\n",
            "Iteration 412, loss = 0.01509596\n",
            "Iteration 413, loss = 0.01507444\n",
            "Iteration 414, loss = 0.01504858\n",
            "Iteration 415, loss = 0.01508515\n",
            "Iteration 416, loss = 0.01497190\n",
            "Iteration 417, loss = 0.01492681\n",
            "Iteration 418, loss = 0.01478186\n",
            "Iteration 419, loss = 0.01479149\n",
            "Iteration 420, loss = 0.01476957\n",
            "Iteration 421, loss = 0.01467063\n",
            "Iteration 422, loss = 0.01476017\n",
            "Iteration 423, loss = 0.01468930\n",
            "Iteration 424, loss = 0.01462983\n",
            "Iteration 425, loss = 0.01460535\n",
            "Iteration 426, loss = 0.01457870\n",
            "Iteration 427, loss = 0.01452034\n",
            "Iteration 428, loss = 0.01450465\n",
            "Iteration 429, loss = 0.01442678\n",
            "Iteration 430, loss = 0.01451462\n",
            "Iteration 431, loss = 0.01434497\n",
            "Iteration 432, loss = 0.01428510\n",
            "Iteration 433, loss = 0.01435812\n",
            "Iteration 434, loss = 0.01435327\n",
            "Iteration 435, loss = 0.01424356\n",
            "Iteration 436, loss = 0.01415387\n",
            "Iteration 437, loss = 0.01413469\n",
            "Iteration 438, loss = 0.01406749\n",
            "Iteration 439, loss = 0.01405441\n",
            "Iteration 440, loss = 0.01407483\n",
            "Iteration 441, loss = 0.01399755\n",
            "Iteration 442, loss = 0.01399153\n",
            "Iteration 443, loss = 0.01391915\n",
            "Iteration 444, loss = 0.01389644\n",
            "Iteration 445, loss = 0.01386855\n",
            "Iteration 446, loss = 0.01379485\n",
            "Iteration 447, loss = 0.01376086\n",
            "Iteration 448, loss = 0.01374898\n",
            "Iteration 449, loss = 0.01374640\n",
            "Iteration 450, loss = 0.01379085\n",
            "Iteration 451, loss = 0.01360034\n",
            "Iteration 452, loss = 0.01380533\n",
            "Iteration 453, loss = 0.01359552\n",
            "Iteration 454, loss = 0.01356507\n",
            "Iteration 455, loss = 0.01357977\n",
            "Iteration 456, loss = 0.01357175\n",
            "Iteration 457, loss = 0.01348328\n",
            "Iteration 458, loss = 0.01339207\n",
            "Iteration 459, loss = 0.01339032\n",
            "Iteration 460, loss = 0.01336433\n",
            "Iteration 461, loss = 0.01333757\n",
            "Iteration 462, loss = 0.01340135\n",
            "Iteration 463, loss = 0.01324376\n",
            "Iteration 464, loss = 0.01327343\n",
            "Iteration 465, loss = 0.01330312\n",
            "Iteration 466, loss = 0.01316537\n",
            "Iteration 467, loss = 0.01312298\n",
            "Iteration 468, loss = 0.01315058\n",
            "Iteration 469, loss = 0.01309272\n",
            "Iteration 470, loss = 0.01302060\n",
            "Iteration 471, loss = 0.01302917\n",
            "Iteration 472, loss = 0.01306239\n",
            "Iteration 473, loss = 0.01313933\n",
            "Iteration 474, loss = 0.01291615\n",
            "Iteration 475, loss = 0.01288491\n",
            "Iteration 476, loss = 0.01287219\n",
            "Iteration 477, loss = 0.01283377\n",
            "Iteration 478, loss = 0.01277155\n",
            "Iteration 479, loss = 0.01280887\n",
            "Iteration 480, loss = 0.01271346\n",
            "Iteration 481, loss = 0.01271460\n",
            "Iteration 482, loss = 0.01269146\n",
            "Iteration 483, loss = 0.01263352\n",
            "Iteration 484, loss = 0.01258065\n",
            "Iteration 485, loss = 0.01257660\n",
            "Iteration 486, loss = 0.01253818\n",
            "Iteration 487, loss = 0.01260000\n",
            "Iteration 488, loss = 0.01251399\n",
            "Iteration 489, loss = 0.01247767\n",
            "Iteration 490, loss = 0.01248224\n",
            "Iteration 491, loss = 0.01248563\n",
            "Iteration 492, loss = 0.01235782\n",
            "Iteration 493, loss = 0.01236288\n",
            "Iteration 494, loss = 0.01229594\n",
            "Iteration 495, loss = 0.01228470\n",
            "Iteration 496, loss = 0.01228593\n",
            "Iteration 497, loss = 0.01224756\n",
            "Iteration 498, loss = 0.01214793\n",
            "Iteration 499, loss = 0.01215391\n",
            "Iteration 500, loss = 0.01220860\n",
            "Iteration 501, loss = 0.01218768\n",
            "Iteration 502, loss = 0.01217306\n",
            "Iteration 503, loss = 0.01203148\n",
            "Iteration 504, loss = 0.01204670\n",
            "Iteration 505, loss = 0.01202078\n",
            "Iteration 506, loss = 0.01197137\n",
            "Iteration 507, loss = 0.01199211\n",
            "Iteration 508, loss = 0.01193141\n",
            "Iteration 509, loss = 0.01187538\n",
            "Iteration 510, loss = 0.01195089\n",
            "Iteration 511, loss = 0.01185166\n",
            "Iteration 512, loss = 0.01186339\n",
            "Iteration 513, loss = 0.01179709\n",
            "Iteration 514, loss = 0.01172626\n",
            "Iteration 515, loss = 0.01171047\n",
            "Iteration 516, loss = 0.01169850\n",
            "Iteration 517, loss = 0.01165658\n",
            "Iteration 518, loss = 0.01164526\n",
            "Iteration 519, loss = 0.01162570\n",
            "Iteration 520, loss = 0.01158534\n",
            "Iteration 521, loss = 0.01152622\n",
            "Iteration 522, loss = 0.01161246\n",
            "Iteration 523, loss = 0.01163793\n",
            "Iteration 524, loss = 0.01152384\n",
            "Iteration 525, loss = 0.01153456\n",
            "Iteration 526, loss = 0.01143879\n",
            "Iteration 527, loss = 0.01142620\n",
            "Iteration 528, loss = 0.01135879\n",
            "Iteration 529, loss = 0.01135792\n",
            "Iteration 530, loss = 0.01138268\n",
            "Iteration 531, loss = 0.01129436\n",
            "Iteration 532, loss = 0.01127555\n",
            "Iteration 533, loss = 0.01132231\n",
            "Iteration 534, loss = 0.01122776\n",
            "Iteration 535, loss = 0.01120622\n",
            "Iteration 536, loss = 0.01129377\n",
            "Iteration 537, loss = 0.01129140\n",
            "Iteration 538, loss = 0.01118683\n",
            "Iteration 539, loss = 0.01108707\n",
            "Iteration 540, loss = 0.01110325\n",
            "Iteration 541, loss = 0.01106021\n",
            "Iteration 542, loss = 0.01103191\n",
            "Iteration 543, loss = 0.01102807\n",
            "Iteration 544, loss = 0.01101889\n",
            "Iteration 545, loss = 0.01105303\n",
            "Iteration 546, loss = 0.01094877\n",
            "Iteration 547, loss = 0.01093448\n",
            "Iteration 548, loss = 0.01090098\n",
            "Iteration 549, loss = 0.01087789\n",
            "Iteration 550, loss = 0.01086882\n",
            "Iteration 551, loss = 0.01086698\n",
            "Iteration 552, loss = 0.01084232\n",
            "Iteration 553, loss = 0.01078898\n",
            "Iteration 554, loss = 0.01075524\n",
            "Iteration 555, loss = 0.01071915\n",
            "Iteration 556, loss = 0.01071874\n",
            "Iteration 557, loss = 0.01067057\n",
            "Iteration 558, loss = 0.01065793\n",
            "Iteration 559, loss = 0.01060937\n",
            "Iteration 560, loss = 0.01064176\n",
            "Iteration 561, loss = 0.01061148\n",
            "Iteration 562, loss = 0.01062892\n",
            "Iteration 563, loss = 0.01051118\n",
            "Iteration 564, loss = 0.01050957\n",
            "Iteration 565, loss = 0.01050137\n",
            "Iteration 566, loss = 0.01047133\n",
            "Iteration 567, loss = 0.01053173\n",
            "Iteration 568, loss = 0.01031122\n",
            "Iteration 569, loss = 0.01052047\n",
            "Iteration 570, loss = 0.01047763\n",
            "Iteration 571, loss = 0.01043879\n",
            "Iteration 572, loss = 0.01037162\n",
            "Iteration 573, loss = 0.01031940\n",
            "Iteration 574, loss = 0.01028801\n",
            "Iteration 575, loss = 0.01033009\n",
            "Iteration 576, loss = 0.01029437\n",
            "Iteration 577, loss = 0.01028565\n",
            "Iteration 578, loss = 0.01021930\n",
            "Iteration 579, loss = 0.01015897\n",
            "Iteration 580, loss = 0.01024753\n",
            "Iteration 581, loss = 0.01020029\n",
            "Iteration 582, loss = 0.01012418\n",
            "Iteration 583, loss = 0.01021242\n",
            "Iteration 584, loss = 0.01009858\n",
            "Iteration 585, loss = 0.01001734\n",
            "Iteration 586, loss = 0.01004072\n",
            "Iteration 587, loss = 0.01000791\n",
            "Iteration 588, loss = 0.00997249\n",
            "Iteration 589, loss = 0.00998192\n",
            "Iteration 590, loss = 0.00997619\n",
            "Iteration 591, loss = 0.00990735\n",
            "Iteration 592, loss = 0.00987564\n",
            "Iteration 593, loss = 0.00994232\n",
            "Iteration 594, loss = 0.00986699\n",
            "Iteration 595, loss = 0.00984187\n",
            "Iteration 596, loss = 0.00985540\n",
            "Iteration 597, loss = 0.00981099\n",
            "Iteration 598, loss = 0.00987331\n",
            "Iteration 599, loss = 0.00987858\n",
            "Iteration 600, loss = 0.00983956\n",
            "Iteration 601, loss = 0.00979100\n",
            "Iteration 602, loss = 0.00990447\n",
            "Iteration 603, loss = 0.00963930\n",
            "Iteration 604, loss = 0.00966098\n",
            "Iteration 605, loss = 0.00967031\n",
            "Iteration 606, loss = 0.00961967\n",
            "Iteration 607, loss = 0.00959007\n",
            "Iteration 608, loss = 0.00963233\n",
            "Iteration 609, loss = 0.00954894\n",
            "Iteration 610, loss = 0.00960303\n",
            "Iteration 611, loss = 0.00952547\n",
            "Iteration 612, loss = 0.00953139\n",
            "Iteration 613, loss = 0.00950339\n",
            "Iteration 614, loss = 0.00952179\n",
            "Iteration 615, loss = 0.00944123\n",
            "Iteration 616, loss = 0.00939769\n",
            "Iteration 617, loss = 0.00937417\n",
            "Iteration 618, loss = 0.00938376\n",
            "Iteration 619, loss = 0.00936097\n",
            "Iteration 620, loss = 0.00942366\n",
            "Iteration 621, loss = 0.00931462\n",
            "Iteration 622, loss = 0.00932947\n",
            "Iteration 623, loss = 0.00927901\n",
            "Iteration 624, loss = 0.00933321\n",
            "Iteration 625, loss = 0.00923579\n",
            "Iteration 626, loss = 0.00923510\n",
            "Iteration 627, loss = 0.00926964\n",
            "Iteration 628, loss = 0.00919946\n",
            "Iteration 629, loss = 0.00918691\n",
            "Iteration 630, loss = 0.00925125\n",
            "Iteration 631, loss = 0.00912477\n",
            "Iteration 632, loss = 0.00919255\n",
            "Iteration 633, loss = 0.00916605\n",
            "Iteration 634, loss = 0.00906581\n",
            "Iteration 635, loss = 0.00907022\n",
            "Iteration 636, loss = 0.00907895\n",
            "Iteration 637, loss = 0.00908531\n",
            "Iteration 638, loss = 0.00906765\n",
            "Iteration 639, loss = 0.00903153\n",
            "Iteration 640, loss = 0.00900678\n",
            "Iteration 641, loss = 0.00895581\n",
            "Iteration 642, loss = 0.00898824\n",
            "Iteration 643, loss = 0.00900680\n",
            "Iteration 644, loss = 0.00892621\n",
            "Iteration 645, loss = 0.00889114\n",
            "Iteration 646, loss = 0.00890074\n",
            "Iteration 647, loss = 0.00892324\n",
            "Iteration 648, loss = 0.00893678\n",
            "Iteration 649, loss = 0.00882611\n",
            "Iteration 650, loss = 0.00883552\n",
            "Iteration 651, loss = 0.00884216\n",
            "Iteration 652, loss = 0.00885590\n",
            "Iteration 653, loss = 0.00885780\n",
            "Iteration 654, loss = 0.00872881\n",
            "Iteration 655, loss = 0.00872893\n",
            "Iteration 656, loss = 0.00866943\n",
            "Iteration 657, loss = 0.00866251\n",
            "Iteration 658, loss = 0.00885499\n",
            "Iteration 659, loss = 0.00871610\n",
            "Iteration 660, loss = 0.00862862\n",
            "Iteration 661, loss = 0.00861751\n",
            "Iteration 662, loss = 0.00863288\n",
            "Iteration 663, loss = 0.00857760\n",
            "Iteration 664, loss = 0.00854125\n",
            "Iteration 665, loss = 0.00860803\n",
            "Iteration 666, loss = 0.00855162\n",
            "Iteration 667, loss = 0.00850350\n",
            "Iteration 668, loss = 0.00849409\n",
            "Iteration 669, loss = 0.00848224\n",
            "Iteration 670, loss = 0.00851562\n",
            "Iteration 671, loss = 0.00844729\n",
            "Iteration 672, loss = 0.00849942\n",
            "Iteration 673, loss = 0.00841455\n",
            "Iteration 674, loss = 0.00840639\n",
            "Iteration 675, loss = 0.00838101\n",
            "Iteration 676, loss = 0.00844053\n",
            "Iteration 677, loss = 0.00835809\n",
            "Iteration 678, loss = 0.00833128\n",
            "Iteration 679, loss = 0.00831080\n",
            "Iteration 680, loss = 0.00830021\n",
            "Iteration 681, loss = 0.00830610\n",
            "Iteration 682, loss = 0.00831974\n",
            "Iteration 683, loss = 0.00824913\n",
            "Iteration 684, loss = 0.00827078\n",
            "Iteration 685, loss = 0.00823842\n",
            "Iteration 686, loss = 0.00836384\n",
            "Iteration 687, loss = 0.00830389\n",
            "Iteration 688, loss = 0.00813915\n",
            "Iteration 689, loss = 0.00816807\n",
            "Iteration 690, loss = 0.00819762\n",
            "Iteration 691, loss = 0.00826311\n",
            "Iteration 692, loss = 0.00812278\n",
            "Iteration 693, loss = 0.00813027\n",
            "Iteration 694, loss = 0.00810997\n",
            "Iteration 695, loss = 0.00811973\n",
            "Iteration 696, loss = 0.00807400\n",
            "Iteration 697, loss = 0.00802630\n",
            "Iteration 698, loss = 0.00809250\n",
            "Iteration 699, loss = 0.00803931\n",
            "Iteration 700, loss = 0.00804813\n",
            "Iteration 701, loss = 0.00803542\n",
            "Iteration 702, loss = 0.00795603\n",
            "Iteration 703, loss = 0.00806329\n",
            "Iteration 704, loss = 0.00796520\n",
            "Iteration 705, loss = 0.00795279\n",
            "Iteration 706, loss = 0.00796168\n",
            "Iteration 707, loss = 0.00789691\n",
            "Iteration 708, loss = 0.00791398\n",
            "Iteration 709, loss = 0.00788258\n",
            "Iteration 710, loss = 0.00785507\n",
            "Iteration 711, loss = 0.00782993\n",
            "Iteration 712, loss = 0.00787669\n",
            "Iteration 713, loss = 0.00780900\n",
            "Iteration 714, loss = 0.00780948\n",
            "Iteration 715, loss = 0.00778425\n",
            "Iteration 716, loss = 0.00774944\n",
            "Iteration 717, loss = 0.00773601\n",
            "Iteration 718, loss = 0.00773379\n",
            "Iteration 719, loss = 0.00773560\n",
            "Iteration 720, loss = 0.00769886\n",
            "Iteration 721, loss = 0.00774990\n",
            "Iteration 722, loss = 0.00774151\n",
            "Iteration 723, loss = 0.00782168\n",
            "Iteration 724, loss = 0.00766389\n",
            "Iteration 725, loss = 0.00769346\n",
            "Iteration 726, loss = 0.00762292\n",
            "Iteration 727, loss = 0.00774702\n",
            "Iteration 728, loss = 0.00764898\n",
            "Iteration 729, loss = 0.00761725\n",
            "Iteration 730, loss = 0.00774897\n",
            "Iteration 731, loss = 0.00767190\n",
            "Iteration 732, loss = 0.00755154\n",
            "Iteration 733, loss = 0.00760572\n",
            "Iteration 734, loss = 0.00750846\n",
            "Iteration 735, loss = 0.00752687\n",
            "Iteration 736, loss = 0.00754456\n",
            "Iteration 737, loss = 0.00751551\n",
            "Iteration 738, loss = 0.00750155\n",
            "Iteration 739, loss = 0.00742987\n",
            "Iteration 740, loss = 0.00739737\n",
            "Iteration 741, loss = 0.00743259\n",
            "Iteration 742, loss = 0.00743696\n",
            "Iteration 743, loss = 0.00744870\n",
            "Iteration 744, loss = 0.00737706\n",
            "Iteration 745, loss = 0.00741710\n",
            "Iteration 746, loss = 0.00726942\n",
            "Iteration 747, loss = 0.00742597\n",
            "Iteration 748, loss = 0.00742372\n",
            "Iteration 749, loss = 0.00737209\n",
            "Iteration 750, loss = 0.00727425\n",
            "Iteration 751, loss = 0.00731814\n",
            "Iteration 752, loss = 0.00728658\n",
            "Iteration 753, loss = 0.00724260\n",
            "Iteration 754, loss = 0.00725613\n",
            "Iteration 755, loss = 0.00730483\n",
            "Iteration 756, loss = 0.00726517\n",
            "Iteration 757, loss = 0.00725724\n",
            "Iteration 758, loss = 0.00722418\n",
            "Iteration 759, loss = 0.00718899\n",
            "Iteration 760, loss = 0.00713590\n",
            "Iteration 761, loss = 0.00715589\n",
            "Iteration 762, loss = 0.00721799\n",
            "Iteration 763, loss = 0.00715612\n",
            "Iteration 764, loss = 0.00713705\n",
            "Iteration 765, loss = 0.00712406\n",
            "Iteration 766, loss = 0.00706518\n",
            "Iteration 767, loss = 0.00706907\n",
            "Iteration 768, loss = 0.00708082\n",
            "Iteration 769, loss = 0.00710702\n",
            "Iteration 770, loss = 0.00701384\n",
            "Iteration 771, loss = 0.00705096\n",
            "Iteration 772, loss = 0.00701803\n",
            "Iteration 773, loss = 0.00702602\n",
            "Iteration 774, loss = 0.00702598\n",
            "Iteration 775, loss = 0.00699372\n",
            "Iteration 776, loss = 0.00698239\n",
            "Iteration 777, loss = 0.00707204\n",
            "Iteration 778, loss = 0.00705999\n",
            "Iteration 779, loss = 0.00699927\n",
            "Iteration 780, loss = 0.00698750\n",
            "Iteration 781, loss = 0.00693659\n",
            "Iteration 782, loss = 0.00690205\n",
            "Iteration 783, loss = 0.00690852\n",
            "Iteration 784, loss = 0.00686800\n",
            "Iteration 785, loss = 0.00695379\n",
            "Iteration 786, loss = 0.00686770\n",
            "Iteration 787, loss = 0.00682543\n",
            "Iteration 788, loss = 0.00680710\n",
            "Iteration 789, loss = 0.00678767\n",
            "Iteration 790, loss = 0.00680672\n",
            "Iteration 791, loss = 0.00676685\n",
            "Iteration 792, loss = 0.00677450\n",
            "Iteration 793, loss = 0.00675487\n",
            "Iteration 794, loss = 0.00674690\n",
            "Iteration 795, loss = 0.00674616\n",
            "Iteration 796, loss = 0.00671714\n",
            "Iteration 797, loss = 0.00672677\n",
            "Iteration 798, loss = 0.00668519\n",
            "Iteration 799, loss = 0.00675548\n",
            "Iteration 800, loss = 0.00670839\n",
            "Iteration 801, loss = 0.00672936\n",
            "Iteration 802, loss = 0.00663385\n",
            "Iteration 803, loss = 0.00666903\n",
            "Iteration 804, loss = 0.00663736\n",
            "Iteration 805, loss = 0.00676077\n",
            "Iteration 806, loss = 0.00669927\n",
            "Iteration 807, loss = 0.00662846\n",
            "Iteration 808, loss = 0.00659829\n",
            "Iteration 809, loss = 0.00660452\n",
            "Iteration 810, loss = 0.00661672\n",
            "Iteration 811, loss = 0.00659774\n",
            "Iteration 812, loss = 0.00656668\n",
            "Iteration 813, loss = 0.00652157\n",
            "Iteration 814, loss = 0.00655195\n",
            "Iteration 815, loss = 0.00652369\n",
            "Iteration 816, loss = 0.00661061\n",
            "Iteration 817, loss = 0.00656217\n",
            "Iteration 818, loss = 0.00657351\n",
            "Iteration 819, loss = 0.00646061\n",
            "Iteration 820, loss = 0.00655543\n",
            "Iteration 821, loss = 0.00644387\n",
            "Iteration 822, loss = 0.00643707\n",
            "Iteration 823, loss = 0.00639121\n",
            "Iteration 824, loss = 0.00643550\n",
            "Iteration 825, loss = 0.00644520\n",
            "Iteration 826, loss = 0.00641689\n",
            "Iteration 827, loss = 0.00636946\n",
            "Iteration 828, loss = 0.00633789\n",
            "Iteration 829, loss = 0.00640207\n",
            "Iteration 830, loss = 0.00647653\n",
            "Iteration 831, loss = 0.00635486\n",
            "Iteration 832, loss = 0.00646910\n",
            "Iteration 833, loss = 0.00642167\n",
            "Iteration 834, loss = 0.00639879\n",
            "Iteration 835, loss = 0.00631031\n",
            "Iteration 836, loss = 0.00635635\n",
            "Iteration 837, loss = 0.00628353\n",
            "Iteration 838, loss = 0.00628306\n",
            "Iteration 839, loss = 0.00627941\n",
            "Iteration 840, loss = 0.00626258\n",
            "Iteration 841, loss = 0.00627735\n",
            "Iteration 842, loss = 0.00618214\n",
            "Iteration 843, loss = 0.00631190\n",
            "Iteration 844, loss = 0.00623021\n",
            "Iteration 845, loss = 0.00624941\n",
            "Iteration 846, loss = 0.00631326\n",
            "Iteration 847, loss = 0.00617729\n",
            "Iteration 848, loss = 0.00612565\n",
            "Iteration 849, loss = 0.00615749\n",
            "Iteration 850, loss = 0.00619990\n",
            "Iteration 851, loss = 0.00615092\n",
            "Iteration 852, loss = 0.00618215\n",
            "Iteration 853, loss = 0.00608895\n",
            "Iteration 854, loss = 0.00615842\n",
            "Iteration 855, loss = 0.00612351\n",
            "Iteration 856, loss = 0.00605991\n",
            "Iteration 857, loss = 0.00607868\n",
            "Iteration 858, loss = 0.00607784\n",
            "Iteration 859, loss = 0.00624355\n",
            "Iteration 860, loss = 0.00603474\n",
            "Iteration 861, loss = 0.00601744\n",
            "Iteration 862, loss = 0.00605530\n",
            "Iteration 863, loss = 0.00603610\n",
            "Iteration 864, loss = 0.00599411\n",
            "Iteration 865, loss = 0.00598675\n",
            "Iteration 866, loss = 0.00608944\n",
            "Iteration 867, loss = 0.00604621\n",
            "Iteration 868, loss = 0.00596966\n",
            "Iteration 869, loss = 0.00592692\n",
            "Iteration 870, loss = 0.00596244\n",
            "Iteration 871, loss = 0.00592391\n",
            "Iteration 872, loss = 0.00593597\n",
            "Iteration 873, loss = 0.00590807\n",
            "Iteration 874, loss = 0.00595889\n",
            "Iteration 875, loss = 0.00590781\n",
            "Iteration 876, loss = 0.00587409\n",
            "Iteration 877, loss = 0.00586331\n",
            "Iteration 878, loss = 0.00587197\n",
            "Iteration 879, loss = 0.00589341\n",
            "Iteration 880, loss = 0.00581639\n",
            "Iteration 881, loss = 0.00595201\n",
            "Iteration 882, loss = 0.00583571\n",
            "Iteration 883, loss = 0.00594345\n",
            "Iteration 884, loss = 0.00581398\n",
            "Iteration 885, loss = 0.00577002\n",
            "Iteration 886, loss = 0.00587444\n",
            "Iteration 887, loss = 0.00579262\n",
            "Iteration 888, loss = 0.00581351\n",
            "Iteration 889, loss = 0.00579716\n",
            "Iteration 890, loss = 0.00576310\n",
            "Iteration 891, loss = 0.00575629\n",
            "Iteration 892, loss = 0.00570865\n",
            "Iteration 893, loss = 0.00576414\n",
            "Iteration 894, loss = 0.00576390\n",
            "Iteration 895, loss = 0.00571717\n",
            "Iteration 896, loss = 0.00569894\n",
            "Iteration 897, loss = 0.00565986\n",
            "Iteration 898, loss = 0.00577414\n",
            "Iteration 899, loss = 0.00575412\n",
            "Iteration 900, loss = 0.00566145\n",
            "Iteration 901, loss = 0.00566065\n",
            "Iteration 902, loss = 0.00563002\n",
            "Iteration 903, loss = 0.00561703\n",
            "Iteration 904, loss = 0.00564485\n",
            "Iteration 905, loss = 0.00561140\n",
            "Iteration 906, loss = 0.00561116\n",
            "Iteration 907, loss = 0.00564007\n",
            "Iteration 908, loss = 0.00561599\n",
            "Iteration 909, loss = 0.00560322\n",
            "Iteration 910, loss = 0.00557403\n",
            "Iteration 911, loss = 0.00555413\n",
            "Iteration 912, loss = 0.00560164\n",
            "Iteration 913, loss = 0.00560473\n",
            "Iteration 914, loss = 0.00562063\n",
            "Iteration 915, loss = 0.00549588\n",
            "Iteration 916, loss = 0.00552364\n",
            "Iteration 917, loss = 0.00557226\n",
            "Iteration 918, loss = 0.00557632\n",
            "Iteration 919, loss = 0.00551546\n",
            "Iteration 920, loss = 0.00551674\n",
            "Iteration 921, loss = 0.00553971\n",
            "Iteration 922, loss = 0.00549674\n",
            "Iteration 923, loss = 0.00550776\n",
            "Iteration 924, loss = 0.00548308\n",
            "Iteration 925, loss = 0.00542146\n",
            "Iteration 926, loss = 0.00543393\n",
            "Iteration 927, loss = 0.00543443\n",
            "Iteration 928, loss = 0.00545654\n",
            "Iteration 929, loss = 0.00541706\n",
            "Iteration 930, loss = 0.00540094\n",
            "Iteration 931, loss = 0.00542349\n",
            "Iteration 932, loss = 0.00541703\n",
            "Iteration 933, loss = 0.00535603\n",
            "Iteration 934, loss = 0.00536711\n",
            "Iteration 935, loss = 0.00541443\n",
            "Iteration 936, loss = 0.00539824\n",
            "Iteration 937, loss = 0.00544632\n",
            "Iteration 938, loss = 0.00534074\n",
            "Iteration 939, loss = 0.00539955\n",
            "Iteration 940, loss = 0.00528888\n",
            "Iteration 941, loss = 0.00533063\n",
            "Iteration 942, loss = 0.00538437\n",
            "Iteration 943, loss = 0.00539965\n",
            "Iteration 944, loss = 0.00531136\n",
            "Iteration 945, loss = 0.00533089\n",
            "Iteration 946, loss = 0.00537204\n",
            "Iteration 947, loss = 0.00526218\n",
            "Iteration 948, loss = 0.00524339\n",
            "Iteration 949, loss = 0.00531711\n",
            "Iteration 950, loss = 0.00521345\n",
            "Iteration 951, loss = 0.00524270\n",
            "Iteration 952, loss = 0.00522276\n",
            "Iteration 953, loss = 0.00523580\n",
            "Iteration 954, loss = 0.00516344\n",
            "Iteration 955, loss = 0.00524187\n",
            "Iteration 956, loss = 0.00520856\n",
            "Iteration 957, loss = 0.00518823\n",
            "Iteration 958, loss = 0.00518764\n",
            "Iteration 959, loss = 0.00520954\n",
            "Iteration 960, loss = 0.00518708\n",
            "Iteration 961, loss = 0.00522221\n",
            "Iteration 962, loss = 0.00511789\n",
            "Iteration 963, loss = 0.00509522\n",
            "Iteration 964, loss = 0.00513173\n",
            "Iteration 965, loss = 0.00519477\n",
            "Iteration 966, loss = 0.00516093\n",
            "Iteration 967, loss = 0.00507510\n",
            "Iteration 968, loss = 0.00512552\n",
            "Iteration 969, loss = 0.00511432\n",
            "Iteration 970, loss = 0.00506888\n",
            "Iteration 971, loss = 0.00513595\n",
            "Iteration 972, loss = 0.00503242\n",
            "Iteration 973, loss = 0.00506198\n",
            "Iteration 974, loss = 0.00505592\n",
            "Iteration 975, loss = 0.00505579\n",
            "Iteration 976, loss = 0.00508968\n",
            "Iteration 977, loss = 0.00504850\n",
            "Iteration 978, loss = 0.00504566\n",
            "Iteration 979, loss = 0.00513625\n",
            "Iteration 980, loss = 0.00504452\n",
            "Iteration 981, loss = 0.00500161\n",
            "Iteration 982, loss = 0.00505451\n",
            "Iteration 983, loss = 0.00497470\n",
            "Iteration 984, loss = 0.00498281\n",
            "Iteration 985, loss = 0.00503821\n",
            "Iteration 986, loss = 0.00495982\n",
            "Iteration 987, loss = 0.00494515\n",
            "Iteration 988, loss = 0.00497023\n",
            "Iteration 989, loss = 0.00501508\n",
            "Iteration 990, loss = 0.00498197\n",
            "Iteration 991, loss = 0.00500527\n",
            "Iteration 992, loss = 0.00512793\n",
            "Iteration 993, loss = 0.00509153\n",
            "Iteration 994, loss = 0.00491009\n",
            "Iteration 995, loss = 0.00490158\n",
            "Iteration 996, loss = 0.00488624\n",
            "Iteration 997, loss = 0.00509604\n",
            "Iteration 998, loss = 0.00501692\n",
            "Iteration 999, loss = 0.00496540\n",
            "Iteration 1000, loss = 0.00494504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV89n5sZtdiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24sUeS_HtdfH",
        "colab_type": "code",
        "outputId": "e5bb4e60-77fb-4d9a-a5cb-ee533b23725b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import seaborn as sns\n",
        "ax = sns.heatmap(matriz,annot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgGpqK69uCTv",
        "colab_type": "code",
        "outputId": "fee72a48-e6bc-42b8-e803-814d3f7c9235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[435,   1],\n",
              "       [  1,  63]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSMVJfslukYe",
        "colab_type": "code",
        "outputId": "941dc064-68db-49ac-b1d4-da8f83d71e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precisao"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usoLQCfCzTFU",
        "colab_type": "text"
      },
      "source": [
        "##2.Base Census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f84rNyv-zVCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classe = base_census.iloc[:, 14].values\n",
        "previsores = base_census.iloc[:, 0:14].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS10NRvWzZaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_previsores = LabelEncoder()\n",
        "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:, 1])\n",
        "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:, 3])\n",
        "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:, 5])\n",
        "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:, 6])\n",
        "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:, 7])\n",
        "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:, 8])\n",
        "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:, 9])\n",
        "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:, 13])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jenw1yXBzcu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Melhores resultados sem o one hot enconder\n",
        "# previsores = pd.get_dummies(pd.DataFrame(previsores), columns=[1,3,5,6,7,8,9,13]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwQV_n2Gzeqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_classe = LabelEncoder()\n",
        "classe = labelencoder_classe.fit_transform(classe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8r8-RvDzg9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-6MQ81lzk9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABYdSut9znae",
        "colab_type": "code",
        "outputId": "17619679-5839-46ac-82b7-777813e1113f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "classificador = MLPClassifier(verbose=True, max_iter=1000, tol=0.00001)\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)\n",
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.42803398\n",
            "Iteration 2, loss = 0.35481058\n",
            "Iteration 3, loss = 0.33543686\n",
            "Iteration 4, loss = 0.32791934\n",
            "Iteration 5, loss = 0.32441136\n",
            "Iteration 6, loss = 0.32273499\n",
            "Iteration 7, loss = 0.32142694\n",
            "Iteration 8, loss = 0.32048426\n",
            "Iteration 9, loss = 0.31962804\n",
            "Iteration 10, loss = 0.31927973\n",
            "Iteration 11, loss = 0.31845286\n",
            "Iteration 12, loss = 0.31777658\n",
            "Iteration 13, loss = 0.31711053\n",
            "Iteration 14, loss = 0.31645915\n",
            "Iteration 15, loss = 0.31591605\n",
            "Iteration 16, loss = 0.31567220\n",
            "Iteration 17, loss = 0.31528865\n",
            "Iteration 18, loss = 0.31499562\n",
            "Iteration 19, loss = 0.31460029\n",
            "Iteration 20, loss = 0.31395113\n",
            "Iteration 21, loss = 0.31351057\n",
            "Iteration 22, loss = 0.31297783\n",
            "Iteration 23, loss = 0.31245751\n",
            "Iteration 24, loss = 0.31210016\n",
            "Iteration 25, loss = 0.31160494\n",
            "Iteration 26, loss = 0.31096660\n",
            "Iteration 27, loss = 0.31027363\n",
            "Iteration 28, loss = 0.31023638\n",
            "Iteration 29, loss = 0.30992603\n",
            "Iteration 30, loss = 0.30938066\n",
            "Iteration 31, loss = 0.30912503\n",
            "Iteration 32, loss = 0.30914138\n",
            "Iteration 33, loss = 0.30852572\n",
            "Iteration 34, loss = 0.30807931\n",
            "Iteration 35, loss = 0.30778703\n",
            "Iteration 36, loss = 0.30705291\n",
            "Iteration 37, loss = 0.30723178\n",
            "Iteration 38, loss = 0.30688776\n",
            "Iteration 39, loss = 0.30634144\n",
            "Iteration 40, loss = 0.30606302\n",
            "Iteration 41, loss = 0.30568369\n",
            "Iteration 42, loss = 0.30560622\n",
            "Iteration 43, loss = 0.30485130\n",
            "Iteration 44, loss = 0.30474013\n",
            "Iteration 45, loss = 0.30431018\n",
            "Iteration 46, loss = 0.30400545\n",
            "Iteration 47, loss = 0.30389056\n",
            "Iteration 48, loss = 0.30363396\n",
            "Iteration 49, loss = 0.30308569\n",
            "Iteration 50, loss = 0.30275408\n",
            "Iteration 51, loss = 0.30262153\n",
            "Iteration 52, loss = 0.30209693\n",
            "Iteration 53, loss = 0.30232533\n",
            "Iteration 54, loss = 0.30194927\n",
            "Iteration 55, loss = 0.30147972\n",
            "Iteration 56, loss = 0.30125263\n",
            "Iteration 57, loss = 0.30084893\n",
            "Iteration 58, loss = 0.30124664\n",
            "Iteration 59, loss = 0.30036837\n",
            "Iteration 60, loss = 0.30041422\n",
            "Iteration 61, loss = 0.29969774\n",
            "Iteration 62, loss = 0.29971722\n",
            "Iteration 63, loss = 0.29974450\n",
            "Iteration 64, loss = 0.29950582\n",
            "Iteration 65, loss = 0.29878984\n",
            "Iteration 66, loss = 0.29891985\n",
            "Iteration 67, loss = 0.29882514\n",
            "Iteration 68, loss = 0.29857964\n",
            "Iteration 69, loss = 0.29889053\n",
            "Iteration 70, loss = 0.29823647\n",
            "Iteration 71, loss = 0.29793077\n",
            "Iteration 72, loss = 0.29786146\n",
            "Iteration 73, loss = 0.29753458\n",
            "Iteration 74, loss = 0.29742466\n",
            "Iteration 75, loss = 0.29716057\n",
            "Iteration 76, loss = 0.29693956\n",
            "Iteration 77, loss = 0.29709144\n",
            "Iteration 78, loss = 0.29651865\n",
            "Iteration 79, loss = 0.29662963\n",
            "Iteration 80, loss = 0.29638413\n",
            "Iteration 81, loss = 0.29620435\n",
            "Iteration 82, loss = 0.29581824\n",
            "Iteration 83, loss = 0.29533068\n",
            "Iteration 84, loss = 0.29520488\n",
            "Iteration 85, loss = 0.29543479\n",
            "Iteration 86, loss = 0.29542790\n",
            "Iteration 87, loss = 0.29458419\n",
            "Iteration 88, loss = 0.29507575\n",
            "Iteration 89, loss = 0.29516182\n",
            "Iteration 90, loss = 0.29467865\n",
            "Iteration 91, loss = 0.29447606\n",
            "Iteration 92, loss = 0.29427544\n",
            "Iteration 93, loss = 0.29400066\n",
            "Iteration 94, loss = 0.29414106\n",
            "Iteration 95, loss = 0.29385913\n",
            "Iteration 96, loss = 0.29350817\n",
            "Iteration 97, loss = 0.29334948\n",
            "Iteration 98, loss = 0.29307962\n",
            "Iteration 99, loss = 0.29287842\n",
            "Iteration 100, loss = 0.29309903\n",
            "Iteration 101, loss = 0.29300755\n",
            "Iteration 102, loss = 0.29252473\n",
            "Iteration 103, loss = 0.29325610\n",
            "Iteration 104, loss = 0.29189061\n",
            "Iteration 105, loss = 0.29226645\n",
            "Iteration 106, loss = 0.29218711\n",
            "Iteration 107, loss = 0.29189256\n",
            "Iteration 108, loss = 0.29187120\n",
            "Iteration 109, loss = 0.29165106\n",
            "Iteration 110, loss = 0.29173423\n",
            "Iteration 111, loss = 0.29165425\n",
            "Iteration 112, loss = 0.29131700\n",
            "Iteration 113, loss = 0.29176103\n",
            "Iteration 114, loss = 0.29089446\n",
            "Iteration 115, loss = 0.29100815\n",
            "Iteration 116, loss = 0.29074912\n",
            "Iteration 117, loss = 0.29082805\n",
            "Iteration 118, loss = 0.29011695\n",
            "Iteration 119, loss = 0.29042498\n",
            "Iteration 120, loss = 0.29026090\n",
            "Iteration 121, loss = 0.29003478\n",
            "Iteration 122, loss = 0.28994032\n",
            "Iteration 123, loss = 0.28974255\n",
            "Iteration 124, loss = 0.28984042\n",
            "Iteration 125, loss = 0.28947129\n",
            "Iteration 126, loss = 0.28971258\n",
            "Iteration 127, loss = 0.28931685\n",
            "Iteration 128, loss = 0.28935380\n",
            "Iteration 129, loss = 0.28926922\n",
            "Iteration 130, loss = 0.28893546\n",
            "Iteration 131, loss = 0.28911930\n",
            "Iteration 132, loss = 0.28903781\n",
            "Iteration 133, loss = 0.28892714\n",
            "Iteration 134, loss = 0.28858056\n",
            "Iteration 135, loss = 0.28872972\n",
            "Iteration 136, loss = 0.28874936\n",
            "Iteration 137, loss = 0.28868756\n",
            "Iteration 138, loss = 0.28842157\n",
            "Iteration 139, loss = 0.28834015\n",
            "Iteration 140, loss = 0.28777879\n",
            "Iteration 141, loss = 0.28808642\n",
            "Iteration 142, loss = 0.28795468\n",
            "Iteration 143, loss = 0.28767129\n",
            "Iteration 144, loss = 0.28790066\n",
            "Iteration 145, loss = 0.28775927\n",
            "Iteration 146, loss = 0.28750140\n",
            "Iteration 147, loss = 0.28776615\n",
            "Iteration 148, loss = 0.28760456\n",
            "Iteration 149, loss = 0.28712392\n",
            "Iteration 150, loss = 0.28732895\n",
            "Iteration 151, loss = 0.28696103\n",
            "Iteration 152, loss = 0.28708649\n",
            "Iteration 153, loss = 0.28679879\n",
            "Iteration 154, loss = 0.28698113\n",
            "Iteration 155, loss = 0.28672279\n",
            "Iteration 156, loss = 0.28686021\n",
            "Iteration 157, loss = 0.28703247\n",
            "Iteration 158, loss = 0.28680380\n",
            "Iteration 159, loss = 0.28641721\n",
            "Iteration 160, loss = 0.28647836\n",
            "Iteration 161, loss = 0.28620035\n",
            "Iteration 162, loss = 0.28620782\n",
            "Iteration 163, loss = 0.28586692\n",
            "Iteration 164, loss = 0.28615107\n",
            "Iteration 165, loss = 0.28641424\n",
            "Iteration 166, loss = 0.28698153\n",
            "Iteration 167, loss = 0.28629798\n",
            "Iteration 168, loss = 0.28561767\n",
            "Iteration 169, loss = 0.28512702\n",
            "Iteration 170, loss = 0.28574570\n",
            "Iteration 171, loss = 0.28558944\n",
            "Iteration 172, loss = 0.28572720\n",
            "Iteration 173, loss = 0.28529219\n",
            "Iteration 174, loss = 0.28476811\n",
            "Iteration 175, loss = 0.28552304\n",
            "Iteration 176, loss = 0.28555396\n",
            "Iteration 177, loss = 0.28507187\n",
            "Iteration 178, loss = 0.28566538\n",
            "Iteration 179, loss = 0.28523874\n",
            "Iteration 180, loss = 0.28478168\n",
            "Iteration 181, loss = 0.28503137\n",
            "Iteration 182, loss = 0.28456698\n",
            "Iteration 183, loss = 0.28487289\n",
            "Iteration 184, loss = 0.28523104\n",
            "Iteration 185, loss = 0.28491865\n",
            "Iteration 186, loss = 0.28400946\n",
            "Iteration 187, loss = 0.28426649\n",
            "Iteration 188, loss = 0.28427751\n",
            "Iteration 189, loss = 0.28386246\n",
            "Iteration 190, loss = 0.28470975\n",
            "Iteration 191, loss = 0.28398135\n",
            "Iteration 192, loss = 0.28392602\n",
            "Iteration 193, loss = 0.28366760\n",
            "Iteration 194, loss = 0.28387000\n",
            "Iteration 195, loss = 0.28365010\n",
            "Iteration 196, loss = 0.28353722\n",
            "Iteration 197, loss = 0.28322578\n",
            "Iteration 198, loss = 0.28355779\n",
            "Iteration 199, loss = 0.28355474\n",
            "Iteration 200, loss = 0.28341492\n",
            "Iteration 201, loss = 0.28355320\n",
            "Iteration 202, loss = 0.28292201\n",
            "Iteration 203, loss = 0.28271370\n",
            "Iteration 204, loss = 0.28349913\n",
            "Iteration 205, loss = 0.28318435\n",
            "Iteration 206, loss = 0.28279306\n",
            "Iteration 207, loss = 0.28270906\n",
            "Iteration 208, loss = 0.28317066\n",
            "Iteration 209, loss = 0.28267920\n",
            "Iteration 210, loss = 0.28266950\n",
            "Iteration 211, loss = 0.28284473\n",
            "Iteration 212, loss = 0.28274155\n",
            "Iteration 213, loss = 0.28231035\n",
            "Iteration 214, loss = 0.28281038\n",
            "Iteration 215, loss = 0.28273386\n",
            "Iteration 216, loss = 0.28245053\n",
            "Iteration 217, loss = 0.28190209\n",
            "Iteration 218, loss = 0.28200988\n",
            "Iteration 219, loss = 0.28236196\n",
            "Iteration 220, loss = 0.28254200\n",
            "Iteration 221, loss = 0.28194280\n",
            "Iteration 222, loss = 0.28257460\n",
            "Iteration 223, loss = 0.28226086\n",
            "Iteration 224, loss = 0.28173083\n",
            "Iteration 225, loss = 0.28206593\n",
            "Iteration 226, loss = 0.28134674\n",
            "Iteration 227, loss = 0.28152566\n",
            "Iteration 228, loss = 0.28154310\n",
            "Iteration 229, loss = 0.28146473\n",
            "Iteration 230, loss = 0.28141642\n",
            "Iteration 231, loss = 0.28185674\n",
            "Iteration 232, loss = 0.28190306\n",
            "Iteration 233, loss = 0.28040437\n",
            "Iteration 234, loss = 0.28105547\n",
            "Iteration 235, loss = 0.28114619\n",
            "Iteration 236, loss = 0.28045740\n",
            "Iteration 237, loss = 0.28077372\n",
            "Iteration 238, loss = 0.28096903\n",
            "Iteration 239, loss = 0.28018451\n",
            "Iteration 240, loss = 0.28099555\n",
            "Iteration 241, loss = 0.28076122\n",
            "Iteration 242, loss = 0.28050949\n",
            "Iteration 243, loss = 0.28088342\n",
            "Iteration 244, loss = 0.28045126\n",
            "Iteration 245, loss = 0.28057089\n",
            "Iteration 246, loss = 0.28023248\n",
            "Iteration 247, loss = 0.28080275\n",
            "Iteration 248, loss = 0.28011125\n",
            "Iteration 249, loss = 0.28021910\n",
            "Iteration 250, loss = 0.28034312\n",
            "Iteration 251, loss = 0.28045298\n",
            "Iteration 252, loss = 0.28001739\n",
            "Iteration 253, loss = 0.28004124\n",
            "Iteration 254, loss = 0.27961960\n",
            "Iteration 255, loss = 0.28004298\n",
            "Iteration 256, loss = 0.27983511\n",
            "Iteration 257, loss = 0.28002221\n",
            "Iteration 258, loss = 0.27943156\n",
            "Iteration 259, loss = 0.27930042\n",
            "Iteration 260, loss = 0.28018313\n",
            "Iteration 261, loss = 0.27952548\n",
            "Iteration 262, loss = 0.27977477\n",
            "Iteration 263, loss = 0.27937858\n",
            "Iteration 264, loss = 0.27920462\n",
            "Iteration 265, loss = 0.27894055\n",
            "Iteration 266, loss = 0.27911230\n",
            "Iteration 267, loss = 0.27927279\n",
            "Iteration 268, loss = 0.27932244\n",
            "Iteration 269, loss = 0.27879071\n",
            "Iteration 270, loss = 0.27884375\n",
            "Iteration 271, loss = 0.27830301\n",
            "Iteration 272, loss = 0.27895575\n",
            "Iteration 273, loss = 0.27889585\n",
            "Iteration 274, loss = 0.27860989\n",
            "Iteration 275, loss = 0.27804969\n",
            "Iteration 276, loss = 0.27834981\n",
            "Iteration 277, loss = 0.27882096\n",
            "Iteration 278, loss = 0.27855354\n",
            "Iteration 279, loss = 0.27838059\n",
            "Iteration 280, loss = 0.27856885\n",
            "Iteration 281, loss = 0.27831423\n",
            "Iteration 282, loss = 0.27803903\n",
            "Iteration 283, loss = 0.27821388\n",
            "Iteration 284, loss = 0.27824445\n",
            "Iteration 285, loss = 0.27783168\n",
            "Iteration 286, loss = 0.27863559\n",
            "Iteration 287, loss = 0.27834865\n",
            "Iteration 288, loss = 0.27759720\n",
            "Iteration 289, loss = 0.27808562\n",
            "Iteration 290, loss = 0.27824791\n",
            "Iteration 291, loss = 0.27758986\n",
            "Iteration 292, loss = 0.27799425\n",
            "Iteration 293, loss = 0.27764021\n",
            "Iteration 294, loss = 0.27777954\n",
            "Iteration 295, loss = 0.27740828\n",
            "Iteration 296, loss = 0.27714273\n",
            "Iteration 297, loss = 0.27751579\n",
            "Iteration 298, loss = 0.27748979\n",
            "Iteration 299, loss = 0.27715446\n",
            "Iteration 300, loss = 0.27748965\n",
            "Iteration 301, loss = 0.27751261\n",
            "Iteration 302, loss = 0.27715939\n",
            "Iteration 303, loss = 0.27788086\n",
            "Iteration 304, loss = 0.27688256\n",
            "Iteration 305, loss = 0.27705107\n",
            "Iteration 306, loss = 0.27706566\n",
            "Iteration 307, loss = 0.27682244\n",
            "Iteration 308, loss = 0.27712193\n",
            "Iteration 309, loss = 0.27703249\n",
            "Iteration 310, loss = 0.27686127\n",
            "Iteration 311, loss = 0.27732550\n",
            "Iteration 312, loss = 0.27661878\n",
            "Iteration 313, loss = 0.27670657\n",
            "Iteration 314, loss = 0.27669041\n",
            "Iteration 315, loss = 0.27663246\n",
            "Iteration 316, loss = 0.27669878\n",
            "Iteration 317, loss = 0.27630048\n",
            "Iteration 318, loss = 0.27665826\n",
            "Iteration 319, loss = 0.27655597\n",
            "Iteration 320, loss = 0.27681066\n",
            "Iteration 321, loss = 0.27642392\n",
            "Iteration 322, loss = 0.27628059\n",
            "Iteration 323, loss = 0.27667501\n",
            "Iteration 324, loss = 0.27624974\n",
            "Iteration 325, loss = 0.27595452\n",
            "Iteration 326, loss = 0.27623809\n",
            "Iteration 327, loss = 0.27634376\n",
            "Iteration 328, loss = 0.27628802\n",
            "Iteration 329, loss = 0.27621205\n",
            "Iteration 330, loss = 0.27630349\n",
            "Iteration 331, loss = 0.27611733\n",
            "Iteration 332, loss = 0.27629785\n",
            "Iteration 333, loss = 0.27641752\n",
            "Iteration 334, loss = 0.27601356\n",
            "Iteration 335, loss = 0.27542605\n",
            "Iteration 336, loss = 0.27702589\n",
            "Iteration 337, loss = 0.27590794\n",
            "Iteration 338, loss = 0.27570728\n",
            "Iteration 339, loss = 0.27592547\n",
            "Iteration 340, loss = 0.27594864\n",
            "Iteration 341, loss = 0.27576521\n",
            "Iteration 342, loss = 0.27493945\n",
            "Iteration 343, loss = 0.27588624\n",
            "Iteration 344, loss = 0.27547629\n",
            "Iteration 345, loss = 0.27588989\n",
            "Iteration 346, loss = 0.27507596\n",
            "Iteration 347, loss = 0.27520331\n",
            "Iteration 348, loss = 0.27508622\n",
            "Iteration 349, loss = 0.27555783\n",
            "Iteration 350, loss = 0.27551048\n",
            "Iteration 351, loss = 0.27481891\n",
            "Iteration 352, loss = 0.27559015\n",
            "Iteration 353, loss = 0.27536121\n",
            "Iteration 354, loss = 0.27516299\n",
            "Iteration 355, loss = 0.27509580\n",
            "Iteration 356, loss = 0.27540142\n",
            "Iteration 357, loss = 0.27404941\n",
            "Iteration 358, loss = 0.27468915\n",
            "Iteration 359, loss = 0.27481196\n",
            "Iteration 360, loss = 0.27516683\n",
            "Iteration 361, loss = 0.27506332\n",
            "Iteration 362, loss = 0.27469396\n",
            "Iteration 363, loss = 0.27461490\n",
            "Iteration 364, loss = 0.27432149\n",
            "Iteration 365, loss = 0.27506920\n",
            "Iteration 366, loss = 0.27448370\n",
            "Iteration 367, loss = 0.27477191\n",
            "Iteration 368, loss = 0.27470394\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XLRJ4VNzsx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "matriz = confusion_matrix(classe_teste, previsoes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm4AVoXOzsvS",
        "colab_type": "code",
        "outputId": "60441761-c15b-4a2e-bbe7-572723134a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matriz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3416,  277],\n",
              "       [ 469,  723]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    }
  ]
}